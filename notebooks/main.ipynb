{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Execution File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Collecting keras<2.16,>=2.15.0 (from tensorflow-macos==2.15.0->tensorflow)\n",
      "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n",
      "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.8.0\n",
      "    Uninstalling keras-3.8.0:\n",
      "      Successfully uninstalled keras-3.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scikeras 0.13.0 requires keras>=3.2.0, but you have keras 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.15.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.keras.models (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow.keras.models\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow.keras.layers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow.keras.layers\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikeras in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (0.13.0)\n",
      "Collecting keras>=3.2.0 (from scikeras)\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikeras) (1.5.1)\n",
      "Requirement already satisfied: absl-py in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (13.7.0)\n",
      "Requirement already satisfied: namex in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.0.7)\n",
      "Requirement already satisfied: h5py in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (3.10.0)\n",
      "Requirement already satisfied: optree in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
      "Requirement already satisfied: ml-dtypes in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (0.2.0)\n",
      "Requirement already satisfied: packaging in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from keras>=3.2.0->scikeras) (23.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from rich->keras>=3.2.0->scikeras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/mwilko777/.pyenv/versions/3.11.0/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install tensorflow.keras.models\n",
    "%pip install tensorflow.keras.layers\n",
    "%pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd, seaborn as sb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Forecasting\n",
    "Firstly, the prediction i am trying to make is Stock Forecasting with data (Jan 2024 - Jan 2025) to ensure the manufacurer has adequate stock for dispatch in time for the next order.\n",
    "\n",
    "#### Notes\n",
    "This would require doing some demand forecasting also (based on previous orders)? More easily done with reoccurant customers who order the same quantity of labels or similar quantities.\n",
    "\n",
    "SP Inv [REL] = RELEASE QUANTITY - Why are some negative, is it a canceled shipment? (negative stock is what goes back to CAT1?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_84417/449238093.py:5: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sp_inventory_a = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] SP Inventory [ADDS].txt', sep='\\t', header=0)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_84417/449238093.py:7: DtypeWarning: Columns (5,29,42,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tickets_c_i = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] Tickets [CUSTOM] [ITEMS].txt', sep='\\t', header=0)\n"
     ]
    }
   ],
   "source": [
    "# convert datasets to csv from txt\n",
    "\n",
    "# \\t used as separator, because of raw data format and the headers as row 0\n",
    "products_s = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] Products [STOCK].txt', sep='\\t', header=0) \n",
    "sp_inventory_a = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] SP Inventory [ADDS].txt', sep='\\t', header=0)\n",
    "sp_inventory_r = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] SP Inventory [REL].txt', sep='\\t', header=0)\n",
    "tickets_c_i = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] Tickets [CUSTOM] [ITEMS].txt', sep='\\t', header=0)\n",
    "tickets_c_m = pd.read_csv('./datasets/stock_forecasting/2022-2025/[LT] Tickets [CUSTOM] [MAIN].txt', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert 'Entered_Date' to datetime format\n",
    "# sp_inventory_a['Entered_Date'] = pd.to_datetime(sp_inventory_a['Entered_Date'], format='%m/%d/%y')\n",
    "# sp_inventory_r['Entered_Date'] = pd.to_datetime(sp_inventory_r['Entered_Date'], format='%m/%d/%y')\n",
    "\n",
    "# # limit the dates to 2024-2025\n",
    "# sp_inventory_a = sp_inventory_a[(sp_inventory_a['Entered_Date'] >= '2024-01-01') & (sp_inventory_a['Entered_Date'] <= '2025-01-01')]\n",
    "# sp_inventory_r = sp_inventory_r[(sp_inventory_r['Entered_Date'] >= '2024-01-01') & (sp_inventory_r['Entered_Date'] <= '2025-01-01')]\n",
    "\n",
    "# # display row number amount\n",
    "# print('SP Inventory Adds: ', sp_inventory_a.shape)\n",
    "# print('SP Inventory Rel: ', sp_inventory_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCTS STOCK\n",
      "   Adhesive,Alternate,Available,BackOrdered,Box_Size,CaseQty,Color,Commission,Cost,Currency_ExchangeRate,Currency_ID,Customer_Num,CustomerName,Desc1,Desc2,EnteredDate,EnteredTime,eTraxx_Forecast_Quantity,eTraxx_Forecast_Range,FC_Cost,File_Name,ID,Inactive,InternetQuery,Inventory_Expires,Is_LinkedCustomPricesUsed,Is_Location_UsedBy_Consignment,Link_Factor,Location,Material,MaxProduce,MinProduce,ModifiedDate,ModifiedTime,newTimeDateStamp,OnOrder,PackageQty,Part_Type,PhysicalInv,PictDisplayFormat,PK_UUID,PriceMode,PriceMode_Local,ProdClass,ProdSubClass,Product_Image_Size,Product_UniqueProdID,Production_Waste,ProductNo,Releases_Allowed_Default,SupplierName,SupplierNo,SupplierNotes,SupplierPartNo,Tag,TotalCost,UPC,Updated,updateTimeDateStamp,Weight\n",
      "0  ,,0,0,,,,0,0.0,0,0,1.0,Catapult,,,06/07/18,09:...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "1  ,,0,0,,,,0,0.0,0,0,1.0,Catapult,This is Andrew...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "2  ,,0,0,,,,0,0.0,0,0,1.0,Catapult,THIS IS LEWIS ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "3  ,,0,0,,,,0,0.0,0,0,1.0,Catapult,CATAPULT STOCK...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "4  ,,0,0,,,,0,49.11,0,-100,16.0,Gem Freshco,El Se...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "PRODUCTS STOCK COLS\n",
      " Index(['Adhesive,Alternate,Available,BackOrdered,Box_Size,CaseQty,Color,Commission,Cost,Currency_ExchangeRate,Currency_ID,Customer_Num,CustomerName,Desc1,Desc2,EnteredDate,EnteredTime,eTraxx_Forecast_Quantity,eTraxx_Forecast_Range,FC_Cost,File_Name,ID,Inactive,InternetQuery,Inventory_Expires,Is_LinkedCustomPricesUsed,Is_Location_UsedBy_Consignment,Link_Factor,Location,Material,MaxProduce,MinProduce,ModifiedDate,ModifiedTime,newTimeDateStamp,OnOrder,PackageQty,Part_Type,PhysicalInv,PictDisplayFormat,PK_UUID,PriceMode,PriceMode_Local,ProdClass,ProdSubClass,Product_Image_Size,Product_UniqueProdID,Production_Waste,ProductNo,Releases_Allowed_Default,SupplierName,SupplierNo,SupplierNotes,SupplierPartNo,Tag,TotalCost,UPC,Updated,updateTimeDateStamp,Weight'], dtype='object')\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output first 5 rows of each dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# change column name of 'ProductNo' to 'ProductNumber'\n",
    "products_s.rename(columns={'ProductNo': 'ProductNumber'}, inplace=True)\n",
    "\n",
    "print('PRODUCTS STOCK\\n', products_s.head())\n",
    "print('PRODUCTS STOCK COLS\\n', products_s.columns)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALES PRICE INV ADD\n",
      "    ID  StockProduct_ID Entered_Date Entered_Time       Entered_By  \\\n",
      "0   1                3     06/16/18     04:46:02   Andrew Burgess   \n",
      "1   2                2     07/10/18     12:13:25   Andrew Burgess   \n",
      "2   3                2     07/10/18     12:19:05   Andrew Burgess   \n",
      "3   4                6     10/11/18     17:43:03       Lewis Cook   \n",
      "4   5                7     10/12/18     09:48:14  Charlie Ellison   \n",
      "\n",
      "  Inventory_Date Inventory_Time  Inventory_DateTime_Stamp Expiration_Date  \\\n",
      "0       06/16/18       04:46:02                 897972362        00/00/00   \n",
      "1       07/10/18       12:13:25                 900072805        00/00/00   \n",
      "2       07/10/18       12:19:05                 900073145        00/00/00   \n",
      "3       10/11/18       17:43:03                 908127783        00/00/00   \n",
      "4       10/12/18       09:48:14                 908185694        00/00/00   \n",
      "\n",
      "   Inventory_Quantity  Inventory_Balance Cost_Unit  Item_Cost Location  \\\n",
      "0              150000                  0      Each        0.0      NaN   \n",
      "1                1000                  0      Each        0.0      NaN   \n",
      "2                1000                  0      Each        0.0      NaN   \n",
      "3             1050000                  0      Each        0.0      NaN   \n",
      "4              155000                  0      Each        0.0      NaN   \n",
      "\n",
      "  Custom_Ticket_ID Custom_Ticket_CustPONum  Source_PONumber  \\\n",
      "0              156                     NaN              NaN   \n",
      "1               97                     NaN              NaN   \n",
      "2               97                     NaN              NaN   \n",
      "3              485                     NaN              NaN   \n",
      "4              487                     NaN              NaN   \n",
      "\n",
      "   Source_POItems_ID  Source_Product_UniqueProdID  Source_PackSlipItem_ID  \\\n",
      "0                NaN                        172.0                     1.0   \n",
      "1                NaN                        102.0                     5.0   \n",
      "2                NaN                        102.0                     5.0   \n",
      "3                NaN                        608.0                   186.0   \n",
      "4                NaN                        609.0                   187.0   \n",
      "\n",
      "                                              Notes Modified_Date  \\\n",
      "0  Custom Ticket Shipped to Stock Product Inventory      00/00/00   \n",
      "1  Custom Ticket Shipped to Stock Product Inventory      00/00/00   \n",
      "2  Custom Ticket Shipped to Stock Product Inventory      02/07/19   \n",
      "3  Custom Ticket Shipped to Stock Product Inventory      00/00/00   \n",
      "4  Custom Ticket Shipped to Stock Product Inventory      00/00/00   \n",
      "\n",
      "  Modified_Time      Modified_By  Modification_Count  Is_User_Made  \\\n",
      "0      00:00:00              NaN                   2         False   \n",
      "1      00:00:00              NaN                   1         False   \n",
      "2      10:42:25  Charlie Ellison                   3         False   \n",
      "3      00:00:00              NaN                   1         False   \n",
      "4      00:00:00              NaN                   2         False   \n",
      "\n",
      "   Releases_Allowed                           PK_UUID Cost_Unit_Local  \n",
      "0                 0  F35AC0C348BE4A47A31B774859F0682F            Each  \n",
      "1                 0  C36373762BAC6C4C9A474098F27D3F27            Each  \n",
      "2                 0  A5ABB2479CE43C459161F89A857EBA85            Each  \n",
      "3                 0  C91FA68BD84CE0419B611D1C3F222E1F            Each  \n",
      "4                 0  FE4A3A73190CED4488A685B7B9C4751B            Each  \n",
      "SALES PRICE INV ADD COLS\n",
      " Index(['ID', 'StockProduct_ID', 'Entered_Date', 'Entered_Time', 'Entered_By',\n",
      "       'Inventory_Date', 'Inventory_Time', 'Inventory_DateTime_Stamp',\n",
      "       'Expiration_Date', 'Inventory_Quantity', 'Inventory_Balance',\n",
      "       'Cost_Unit', 'Item_Cost', 'Location', 'Custom_Ticket_ID',\n",
      "       'Custom_Ticket_CustPONum', 'Source_PONumber', 'Source_POItems_ID',\n",
      "       'Source_Product_UniqueProdID', 'Source_PackSlipItem_ID', 'Notes',\n",
      "       'Modified_Date', 'Modified_Time', 'Modified_By', 'Modification_Count',\n",
      "       'Is_User_Made', 'Releases_Allowed', 'PK_UUID', 'Cost_Unit_Local'],\n",
      "      dtype='object')\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SALES PRICE INV ADD\\n', sp_inventory_a.head())\n",
    "print('SALES PRICE INV ADD COLS\\n', sp_inventory_a.columns)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SALES PRICE INV REL\n",
      "   Cost_Unit Cost_Unit_Local       Entered_By Entered_Date Entered_Time  ID  \\\n",
      "0      Each            Each   Andrew Burgess     06/16/18     06:02:09   1   \n",
      "1      Each            Each   Andrew Burgess     07/10/18     12:18:24   2   \n",
      "2      Each            Each  Charlie Ellison     10/12/18     10:04:21   3   \n",
      "3      Each            Each  Charlie Ellison     10/24/18     13:34:47   4   \n",
      "4      Each            Each  Charlie Ellison     10/24/18     13:35:18   5   \n",
      "\n",
      "   Is_User_Made  Item_Cost  Kit_Part_Number  Kit_Product_ID Location  \\\n",
      "0         False        0.0              NaN             NaN      NaN   \n",
      "1         False        0.0              NaN             NaN      NaN   \n",
      "2         False        0.0              NaN             NaN      NaN   \n",
      "3         False        0.0              NaN             NaN      NaN   \n",
      "4         False        0.0              NaN             NaN      NaN   \n",
      "\n",
      "   Modification_Count Modified_By Modified_Date Modified_Time  \\\n",
      "0                   0         NaN      00/00/00      00:00:00   \n",
      "1                   0         NaN      00/00/00      00:00:00   \n",
      "2                   0         NaN      00/00/00      00:00:00   \n",
      "3                   0         NaN      00/00/00      00:00:00   \n",
      "4                   0         NaN      00/00/00      00:00:00   \n",
      "\n",
      "                                               Notes  PackSlipItem_ID  \\\n",
      "0                         Stock Product Packing Slip              2.0   \n",
      "1  Reissued Packing Slip of Custom Ticket for Sto...              5.0   \n",
      "2                         Stock Product Packing Slip            188.0   \n",
      "3  Reissued Packing Slip of Custom Ticket for Sto...            226.0   \n",
      "4  Reissued Packing Slip of Custom Ticket for Sto...            227.0   \n",
      "\n",
      "                            PK_UUID Release_Date  Release_DateTime_Stamp  \\\n",
      "0  801EBDD7D263BA4CACB45F079ADB665C     06/16/18               897976929   \n",
      "1  57257715D6F3D84FA60D12E1C0D5C2CB     07/10/18               900073104   \n",
      "2  221EC1CD8A8DEE448E246E1FCDCC17A7     10/12/18               908186661   \n",
      "3  816DA5D9912652479C3DF1D3C8EC4499     10/24/18               909236087   \n",
      "4  E16BAD1EBB007846963A95DAA67A1CD6     10/24/18               909236118   \n",
      "\n",
      "   Release_Quantity Release_Time  SP_Inventory_Add_ID  StockProduct_ID  \\\n",
      "0             50000     06:02:09                    1                3   \n",
      "1              1000     12:18:24                    2                2   \n",
      "2             50000     10:04:21                    5                7   \n",
      "3            300000     13:34:47                    7               10   \n",
      "4            400000     13:35:18                    8               14   \n",
      "\n",
      "   StockProduct_Ticket_ID  \n",
      "0                   157.0  \n",
      "1                    97.0  \n",
      "2                   488.0  \n",
      "3                   437.0  \n",
      "4                   441.0  \n",
      "SALES PRICE INV REL COLS\n",
      " Index(['Cost_Unit', 'Cost_Unit_Local', 'Entered_By', 'Entered_Date',\n",
      "       'Entered_Time', 'ID', 'Is_User_Made', 'Item_Cost', 'Kit_Part_Number',\n",
      "       'Kit_Product_ID', 'Location', 'Modification_Count', 'Modified_By',\n",
      "       'Modified_Date', 'Modified_Time', 'Notes', 'PackSlipItem_ID', 'PK_UUID',\n",
      "       'Release_Date', 'Release_DateTime_Stamp', 'Release_Quantity',\n",
      "       'Release_Time', 'SP_Inventory_Add_ID', 'StockProduct_ID',\n",
      "       'StockProduct_Ticket_ID'],\n",
      "      dtype='object')\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SALES PRICE INV REL\\n', sp_inventory_r.head())\n",
    "print('SALES PRICE INV REL COLS\\n', sp_inventory_r.columns)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TICKETS CUSTOM ITEMS\n",
      "      Art  Art_Item  Art_Ticket  Assigned                        ColorDescr  \\\n",
      "0  False     False       False       NaN         C: WHITE, CMYOK, LAM ADH    \n",
      "1  False     False       False       NaN              C: MYK, GLOSS 54121    \n",
      "2  False     False       False       NaN  C: CMYK,7512C,363C,286C,LAM ADH    \n",
      "3  False     False       False       NaN  C: CMYK,7512C,363C,286C,LAM ADH    \n",
      "4  False     False       False       NaN                C: CMYK, GLOSS V     \n",
      "\n",
      "  ConsecNo  CostM  Desc2                                   Description  \\\n",
      "0      NaN      0    NaN            510016W Gumout All In One FSC 10oz   \n",
      "1      NaN      0    NaN       510022W Gumout Octane Booster 10oz Wrap   \n",
      "2      NaN      0    NaN   Simply Nature 93/7 Ground Beef Famil Pack B   \n",
      "3      NaN      0    NaN  Simply Nature 93/7 Ground Beef Family Pack F   \n",
      "4      NaN      0    NaN      72969 FL Ginger Sweet Chili Sauce 14.5oz   \n",
      "\n",
      "   ediLineNumber  Equip_NoColors  Equip_NoFloods  Equip_Null_Cycles  \\\n",
      "0            NaN               0               0                  0   \n",
      "1            NaN               0               0                  0   \n",
      "2            NaN               0               0                  0   \n",
      "3            NaN               0               0                  0   \n",
      "4            NaN               0               0                  0   \n",
      "\n",
      "   Equip3_NoColors  Equip3_NoFloods  Equip3_Null_Cycles  Equip4_NoColors  \\\n",
      "0                0                0                   0                0   \n",
      "1                0                0                   0                0   \n",
      "2                0                0                   0                0   \n",
      "3                0                0                   0                0   \n",
      "4                0                0                   0                0   \n",
      "\n",
      "   Equip4_NoFloods  Equip4_Null_Cycles  eTraxx_Customer_Notes  FC_LineTotal  \\\n",
      "0                0                   0                    NaN             0   \n",
      "1                0                   0                    NaN             0   \n",
      "2                0                   0                    NaN             0   \n",
      "3                0                   0                    NaN             0   \n",
      "4                0                   0                    NaN             0   \n",
      "\n",
      "   FC_PriceM      ID JobType JobType_Local  Line_Weight  LineTotal  Location  \\\n",
      "0          0  240179    RRNC          RRNC            0    2405.70       NaN   \n",
      "1          0  240177    RRNC          RRNC            0    4482.72       NaN   \n",
      "2          0  240176     New           New            0       0.00       NaN   \n",
      "3          0  240175     New           New            0       0.00       NaN   \n",
      "4          0  240170    RRNC          RRNC            0   30382.00       NaN   \n",
      "\n",
      "   MachineCount          newTimeDateStamp  NoColors  NoFloods  OrderQuantity  \\\n",
      "0         10500  2024-11-07T15:09:21.975Z         7         0          54000   \n",
      "1         28000  2024-11-07T15:02:06.176Z         4         0         144000   \n",
      "2         20797  2024-11-07T14:53:12.254Z         8         0         242000   \n",
      "3         18276  2024-11-07T14:52:24.188Z         8         0         242000   \n",
      "4          2807  2024-11-07T14:37:47.499Z         5         0          22000   \n",
      "\n",
      "                            PK_UUID  Plate  Plate_Item  Plate_Ticket  \\\n",
      "0  66113B02D549484EB35E9AB9DC029F15  False       False         False   \n",
      "1  7D0B81A93ADC644E967791BF46A21510  False       False         False   \n",
      "2  23019CC4362EE4449B4F7344589556A8  False       False         False   \n",
      "3  8D221A44A89ECF4580850051B3C12E03  False       False         False   \n",
      "4  48C44D422B580941B6369EC7A20DEB4B  False       False         False   \n",
      "\n",
      "   PO_Number  Press_Null_Cycles   PriceM PriceMode PriceMode_Local  \\\n",
      "0        NaN                  0    44.55     Per M           Per M   \n",
      "1        NaN                  0    31.13     Per M           Per M   \n",
      "2        NaN                  0     0.00     Per M           Per M   \n",
      "3        NaN                  0     0.00     Per M           Per M   \n",
      "4        NaN                  0  1381.00     Per M           Per M   \n",
      "\n",
      "  ProdRevisionNo  ProductNumber  Proof  Proof_Item Proof_Out  Proof_Ticket  \\\n",
      "0            NaN  ITW-033-0041T  False       False  00/00/00         False   \n",
      "1            NaN  ITW-033-0039Y  False       False  00/00/00         False   \n",
      "2            NaN  VER-007-0046Z  False       False  00/00/00         False   \n",
      "3            NaN  VER-007-0045Z  False       False  00/00/00         False   \n",
      "4            NaN  GOL-005-0324Z  False       False  00/00/00         False   \n",
      "\n",
      "  StckPrdShipStat  StockProductID  TicketNumber  UniquePrice  UniqueProdID  \\\n",
      "0             NaN             NaN         99999        False       11704.0   \n",
      "1             NaN             NaN         99997        False        7728.0   \n",
      "2             NaN             NaN         99996        False       28747.0   \n",
      "3             NaN             NaN         99995        False       28746.0   \n",
      "4             NaN             NaN         99993        False       10474.0   \n",
      "\n",
      "   Unit_Weight       updateTimeDateStamp   Work_Status  \n",
      "0            0  2025-01-21T10:00:53.048Z           NaN  \n",
      "1            0  2025-01-21T09:59:44.079Z           NaN  \n",
      "2            0  2025-01-20T20:26:55.057Z      Inactive  \n",
      "3            0  2024-11-13T17:45:49.909Z  8. In Plates  \n",
      "4            0  2024-11-07T14:38:30.627Z           NaN  \n",
      "TICKETS CUSTOM ITEMS COLS\n",
      " Index(['Art', 'Art_Item', 'Art_Ticket', 'Assigned', 'ColorDescr', 'ConsecNo',\n",
      "       'CostM', 'Desc2', 'Description', 'ediLineNumber', 'Equip_NoColors',\n",
      "       'Equip_NoFloods', 'Equip_Null_Cycles', 'Equip3_NoColors',\n",
      "       'Equip3_NoFloods', 'Equip3_Null_Cycles', 'Equip4_NoColors',\n",
      "       'Equip4_NoFloods', 'Equip4_Null_Cycles', 'eTraxx_Customer_Notes',\n",
      "       'FC_LineTotal', 'FC_PriceM', 'ID', 'JobType', 'JobType_Local',\n",
      "       'Line_Weight', 'LineTotal', 'Location', 'MachineCount',\n",
      "       'newTimeDateStamp', 'NoColors', 'NoFloods', 'OrderQuantity', 'PK_UUID',\n",
      "       'Plate', 'Plate_Item', 'Plate_Ticket', 'PO_Number', 'Press_Null_Cycles',\n",
      "       'PriceM', 'PriceMode', 'PriceMode_Local', 'ProdRevisionNo',\n",
      "       'ProductNumber', 'Proof', 'Proof_Item', 'Proof_Out', 'Proof_Ticket',\n",
      "       'StckPrdShipStat', 'StockProductID', 'TicketNumber', 'UniquePrice',\n",
      "       'UniqueProdID', 'Unit_Weight', 'updateTimeDateStamp', 'Work_Status'],\n",
      "      dtype='object')\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TICKETS CUSTOM ITEMS\\n', tickets_c_i.head())\n",
    "print('TICKETS CUSTOM ITEMS COLS\\n', tickets_c_i.columns)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TICKETS CUSTOM MAIN\n",
      "   Act_MakeReady_Footage,Act_OTHER_Hours,ActArtwork,ActFinMaterial,ActFootage,ActMRHrs,ActPackHrs,ActPostPressHours,ActPressSpd,ActQuantity,ActRunHrs,ActStockCost,ActTotalCost,ActualBillings_NetOfSalesTax,ActualCommissionsCost,ActualFanFoldCost,ActualFanfoldHours,ActualFanfoldRate,ActualFootage_StockRolls,ActualGrossMargin_Dollars,ActualGrossMargin_Percent,ActualMSI_StockRolls,ActualNumOfStockRolls,ActualOtherLaborCost,ActualPackagingRate,ActualPackingLaborCost,ActualPostPressLaborCost,ActualPressCost,ActualPressHours,ActualPressRate,ActualRewindingCost,ActualRewindingHours,ActualRewindingRate,ActualTotalFinishing,ActualTotalHours,ActualTotalLaborCosts,ActualTotalMatAndFreightCost,ActualTotalPOCosts,ActWuHrs,AmortizeColorChanges,AmortizePlateChanges,Are_Tools_for_Equip,ArtDone,ArtStat,AutoAppl,BackStage_ColorStrategy,BackStage_DefaultReportForm,BackStage_SmartMarkSet,Bill_Address_ID,Bill_TaxRegion_ID,BillAddr1,BillAddr2,BillCity,BillCountry,BillCounty,BillLocation,BillState,BillZip,Calculation_TimeStamp,CarrierWidth,ColorChangeCost,ColorChangeCost_Extended,ColSpace,ColumnPerf,ConsecNo,CoreSize,CoreType,CornerRadius,CreditHoldOverride,CSA,Currency_ExchangeRate,Currency_ID,Currency_Rate_ID,CustContact_ID,Customer_Total,CustomerName,CustomerNum,CustPONum,DateDone,DateShipped,Due_on_Site_Date,EarlyShipOK,EndUserName,EndUserNum,EndUserPO,EntryBy,EntryDate,EntryTime,Equip_Actual_Cost,Equip_Actual_Hours,Equip_Actual_Length,Equip_Actual_MR_Hours,Equip_Actual_MR_Length,Equip_Actual_Rate,Equip_Actual_Run_Hours,Equip_Actual_Speed,Equip_Actual_WU_Hours,Equip_Done,Equip_EstRunHrs,Equip_EstSpeed,Equip_EstTime,Equip_ID,Equip_MakeReadyHours,Equip_NoAcross,Equip_NoAround,Equip_NumUp_Multiplier,Equip_Status,Equip_WashUpHours,Equip3_Actual_Cost,Equip3_Actual_Hours,Equip3_Actual_Length,Equip3_Actual_MR_Hours,Equip3_Actual_MR_Length,Equip3_Actual_Rate,Equip3_Actual_Run_Hours,Equip3_Actual_Speed,Equip3_Actual_WU_Hours,Equip3_Done,Equip3_EstRunHrs,Equip3_EstSpeed,Equip3_EstTime,Equip3_ID,Equip3_MakeReadyHours,Equip3_NoAcross,Equip3_NoAround,Equip3_NumUp_Multiplier,Equip3_Status,Equip3_WashUpHours,Equip4_Actual_Cost,Equip4_Actual_Hours,Equip4_Actual_Length,Equip4_Actual_MR_Hours,Equip4_Actual_MR_Length,Equip4_Actual_Rate,Equip4_Actual_Run_Hours,Equip4_Actual_Speed,Equip4_Actual_WU_Hours,Equip4_Done,Equip4_EstRunHrs,Equip4_EstSpeed,Equip4_EstTime,Equip4_ID,Equip4_MakeReadyHours,Equip4_NoAcross,Equip4_NoAround,Equip4_NumUp_Multiplier,Equip4_Status,Equip4_WashUpHours,ES_CSR,ES_SalesRep,ESC_Art,ESC_Art_Sales,ESC_Equip,ESC_Equip_Sales,ESC_Equip3,ESC_Equip3_Sales,ESC_Equip4,ESC_Equip4_Sales,ESC_Finish,ESC_Finish_Sales,ESC_Ink,ESC_Ink_Sales,ESC_Plate,ESC_Plate_Sales,ESC_Press,ESC_Press_Sales,ESC_Proof,ESC_Proof_sales,ESC_Ship,ESC_Ship_Sales,ESC_Stock,ESC_Stock_Sales,ESC_TickStat,ESC_TickStat_Sales,ESC_Tool,ESC_Tool_Sales,ESS_Ship,ESS_Ship_Sales,ESS_TickStat,ESS_TickStat_Sales,Est_SetupFootage,Est_SpoilFootage,Est_v_Act_Notes,EstArtwork,EstFinHrs,EstFinMaterial,EstFootage,EstMRHrs,EstPackHrs,EstPostPressHours,EstPressSpd,EstPressTime,EstRunHrs,EstStockCost,EstTime,EstTotal,EstWuHrs,FC_ColorChangeCost,FC_ColorChangeCost_Extended,FC_Customer_Total,FC_EstTotal,FC_MiscCharge,FC_MiscCharge1,FC_MiscCharge2,FC_MiscCharge3,FC_MiscCharge4,FC_PlateChangeCost,FC_PlateChangeCost_Extended,FC_POTotal,FinalUnwind,FinishDone,FinishNotes,FinishStat,FinishType,FlexPack_Gusset,FlexPack_Height,FlexPack_LeftTrim,FlexPack_RightTrim,FlexPack_Type,Frames_Lead_In,Frames_Lead_Out,Freight_AcctNo,GeneralDescr,ID,Image_Rotation,Ink_Status,Internet_Submission,Is_ActBillNetTax_UserModified,Is_AutoOrder,Is_Ink_In,IsPrintReversed,ITSAssocNum,JDF_Note_to_DFE,JDF_Send_Msg,JDF_Sent_On,LabelRepeat,LabelsPer_,LabelsPerFold,LastModified,MainTool,MFGRepName,MFGRepNum,MiscCharge,MiscCharge1,MiscCharge2,MiscCharge3,MiscCharge4,MiscChargeDesc,MiscChargeDesc1,MiscChargeDesc2,MiscChargeDesc3,MiscChargeDesc4,ModifyBy,ModifyDate,ModifyTime,newTimeDateStamp,NoAcross,NoArounPlate,NoColorChanges,NoLabAcrossFin,NoPlateChanges,Number,OnTime,OrderDate,OTSAssocNum,OutsideDiameter,OverRun,Pinfeed,PK_UUID,PlateChangeCost,PlateChangeCost_Extended,PlateDone,PlateStat,POTotal,Press,PressDone,PressStat,PrevJobNum,PriceMode,Priority,ProofDone,ProofStat,RewindEquipNam,RewindEquipNum,RollLength,RollUnit,Roto_CEL_Product_ID,Roto_Quote_Line_ID,Roto_Quote_Number,RowPerf,RowSpace,SalesCommission,Schedule_Status,Screen_Ratio,Shape,Sheet_Height,Sheet_Width,SheetPacktype,Ship_Address_ID,Ship_by_Date,Ship_TaxRegion_ID,ShipAddr1,ShipAddr2,ShipAsOne,ShipAttn_EmailAddress,ShipCity,ShipCountry,ShipCounty,ShipLocation,ShippingInstruc,ShippingStatus,ShipSt,ShipStat,ShipVia,ShipZip,ShrinkSleeve_CutHeight,ShrinkSleeve_LayFlat,ShrinkSleeve_OverLap,SizeAcross,SizeAround,SlitOnRewind,SoldToEndUser,Stock_Allocated,StockDesc1,StockDesc2,StockDesc3,StockIn,StockIn_Local,StockNum1,StockNum2,StockNum3,StockProdDiscnt,StockTicketType,StockWidth1,StockWidth2,StockWidth3,SubTicket,Tab,TabPosition,Tag,Tape,Terms,TicketStatus,TicketStatus_Local,TicketType,TicketType_Local,TicQuantity,Tool_NumberAround,Tool2Descr,Tool3Descr,Tool4Descr,Tool5Descr,ToolNo2,ToolNo3,ToolNo4,ToolNo5,ToolsIn,ToolStat,Total_LineWeight,TotalOrderWeight,TotalShipWeight,TurnBar,UL,updateTimeDateStamp,Use_TurretRewinder,UserDef_MR_1,UserDef_MR_1_Lb,UserDef_MR_2,UserDef_MR_2_Lb\n",
      "0  0,0,0,0,0,0.0,0,0,0,0,0.0,0.0,0.0,0.0,0,0,0,0,...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "1  0,0,0,0,0,0.0,0,0,0,0,0.0,0.0,0.0,0.0,0,0,0,0,...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "2  0,0,0,0,0,0.0,0,0,0,0,0.0,0.0,0.0,0.0,0,0,0,0,...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "3  0,0,0,0,0,0.0,0,0,0,0,0.0,0.0,384.75,0.0,0,0,0...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "4  0,0,0,0,0,0.0,0,0,0,0,0.0,0.0,62.7,0.0,0,0,0,0...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "TICKETS CUSTOM MAIN COLS\n",
      " Index(['Act_MakeReady_Footage,Act_OTHER_Hours,ActArtwork,ActFinMaterial,ActFootage,ActMRHrs,ActPackHrs,ActPostPressHours,ActPressSpd,ActQuantity,ActRunHrs,ActStockCost,ActTotalCost,ActualBillings_NetOfSalesTax,ActualCommissionsCost,ActualFanFoldCost,ActualFanfoldHours,ActualFanfoldRate,ActualFootage_StockRolls,ActualGrossMargin_Dollars,ActualGrossMargin_Percent,ActualMSI_StockRolls,ActualNumOfStockRolls,ActualOtherLaborCost,ActualPackagingRate,ActualPackingLaborCost,ActualPostPressLaborCost,ActualPressCost,ActualPressHours,ActualPressRate,ActualRewindingCost,ActualRewindingHours,ActualRewindingRate,ActualTotalFinishing,ActualTotalHours,ActualTotalLaborCosts,ActualTotalMatAndFreightCost,ActualTotalPOCosts,ActWuHrs,AmortizeColorChanges,AmortizePlateChanges,Are_Tools_for_Equip,ArtDone,ArtStat,AutoAppl,BackStage_ColorStrategy,BackStage_DefaultReportForm,BackStage_SmartMarkSet,Bill_Address_ID,Bill_TaxRegion_ID,BillAddr1,BillAddr2,BillCity,BillCountry,BillCounty,BillLocation,BillState,BillZip,Calculation_TimeStamp,CarrierWidth,ColorChangeCost,ColorChangeCost_Extended,ColSpace,ColumnPerf,ConsecNo,CoreSize,CoreType,CornerRadius,CreditHoldOverride,CSA,Currency_ExchangeRate,Currency_ID,Currency_Rate_ID,CustContact_ID,Customer_Total,CustomerName,CustomerNum,CustPONum,DateDone,DateShipped,Due_on_Site_Date,EarlyShipOK,EndUserName,EndUserNum,EndUserPO,EntryBy,EntryDate,EntryTime,Equip_Actual_Cost,Equip_Actual_Hours,Equip_Actual_Length,Equip_Actual_MR_Hours,Equip_Actual_MR_Length,Equip_Actual_Rate,Equip_Actual_Run_Hours,Equip_Actual_Speed,Equip_Actual_WU_Hours,Equip_Done,Equip_EstRunHrs,Equip_EstSpeed,Equip_EstTime,Equip_ID,Equip_MakeReadyHours,Equip_NoAcross,Equip_NoAround,Equip_NumUp_Multiplier,Equip_Status,Equip_WashUpHours,Equip3_Actual_Cost,Equip3_Actual_Hours,Equip3_Actual_Length,Equip3_Actual_MR_Hours,Equip3_Actual_MR_Length,Equip3_Actual_Rate,Equip3_Actual_Run_Hours,Equip3_Actual_Speed,Equip3_Actual_WU_Hours,Equip3_Done,Equip3_EstRunHrs,Equip3_EstSpeed,Equip3_EstTime,Equip3_ID,Equip3_MakeReadyHours,Equip3_NoAcross,Equip3_NoAround,Equip3_NumUp_Multiplier,Equip3_Status,Equip3_WashUpHours,Equip4_Actual_Cost,Equip4_Actual_Hours,Equip4_Actual_Length,Equip4_Actual_MR_Hours,Equip4_Actual_MR_Length,Equip4_Actual_Rate,Equip4_Actual_Run_Hours,Equip4_Actual_Speed,Equip4_Actual_WU_Hours,Equip4_Done,Equip4_EstRunHrs,Equip4_EstSpeed,Equip4_EstTime,Equip4_ID,Equip4_MakeReadyHours,Equip4_NoAcross,Equip4_NoAround,Equip4_NumUp_Multiplier,Equip4_Status,Equip4_WashUpHours,ES_CSR,ES_SalesRep,ESC_Art,ESC_Art_Sales,ESC_Equip,ESC_Equip_Sales,ESC_Equip3,ESC_Equip3_Sales,ESC_Equip4,ESC_Equip4_Sales,ESC_Finish,ESC_Finish_Sales,ESC_Ink,ESC_Ink_Sales,ESC_Plate,ESC_Plate_Sales,ESC_Press,ESC_Press_Sales,ESC_Proof,ESC_Proof_sales,ESC_Ship,ESC_Ship_Sales,ESC_Stock,ESC_Stock_Sales,ESC_TickStat,ESC_TickStat_Sales,ESC_Tool,ESC_Tool_Sales,ESS_Ship,ESS_Ship_Sales,ESS_TickStat,ESS_TickStat_Sales,Est_SetupFootage,Est_SpoilFootage,Est_v_Act_Notes,EstArtwork,EstFinHrs,EstFinMaterial,EstFootage,EstMRHrs,EstPackHrs,EstPostPressHours,EstPressSpd,EstPressTime,EstRunHrs,EstStockCost,EstTime,EstTotal,EstWuHrs,FC_ColorChangeCost,FC_ColorChangeCost_Extended,FC_Customer_Total,FC_EstTotal,FC_MiscCharge,FC_MiscCharge1,FC_MiscCharge2,FC_MiscCharge3,FC_MiscCharge4,FC_PlateChangeCost,FC_PlateChangeCost_Extended,FC_POTotal,FinalUnwind,FinishDone,FinishNotes,FinishStat,FinishType,FlexPack_Gusset,FlexPack_Height,FlexPack_LeftTrim,FlexPack_RightTrim,FlexPack_Type,Frames_Lead_In,Frames_Lead_Out,Freight_AcctNo,GeneralDescr,ID,Image_Rotation,Ink_Status,Internet_Submission,Is_ActBillNetTax_UserModified,Is_AutoOrder,Is_Ink_In,IsPrintReversed,ITSAssocNum,JDF_Note_to_DFE,JDF_Send_Msg,JDF_Sent_On,LabelRepeat,LabelsPer_,LabelsPerFold,LastModified,MainTool,MFGRepName,MFGRepNum,MiscCharge,MiscCharge1,MiscCharge2,MiscCharge3,MiscCharge4,MiscChargeDesc,MiscChargeDesc1,MiscChargeDesc2,MiscChargeDesc3,MiscChargeDesc4,ModifyBy,ModifyDate,ModifyTime,newTimeDateStamp,NoAcross,NoArounPlate,NoColorChanges,NoLabAcrossFin,NoPlateChanges,Number,OnTime,OrderDate,OTSAssocNum,OutsideDiameter,OverRun,Pinfeed,PK_UUID,PlateChangeCost,PlateChangeCost_Extended,PlateDone,PlateStat,POTotal,Press,PressDone,PressStat,PrevJobNum,PriceMode,Priority,ProofDone,ProofStat,RewindEquipNam,RewindEquipNum,RollLength,RollUnit,Roto_CEL_Product_ID,Roto_Quote_Line_ID,Roto_Quote_Number,RowPerf,RowSpace,SalesCommission,Schedule_Status,Screen_Ratio,Shape,Sheet_Height,Sheet_Width,SheetPacktype,Ship_Address_ID,Ship_by_Date,Ship_TaxRegion_ID,ShipAddr1,ShipAddr2,ShipAsOne,ShipAttn_EmailAddress,ShipCity,ShipCountry,ShipCounty,ShipLocation,ShippingInstruc,ShippingStatus,ShipSt,ShipStat,ShipVia,ShipZip,ShrinkSleeve_CutHeight,ShrinkSleeve_LayFlat,ShrinkSleeve_OverLap,SizeAcross,SizeAround,SlitOnRewind,SoldToEndUser,Stock_Allocated,StockDesc1,StockDesc2,StockDesc3,StockIn,StockIn_Local,StockNum1,StockNum2,StockNum3,StockProdDiscnt,StockTicketType,StockWidth1,StockWidth2,StockWidth3,SubTicket,Tab,TabPosition,Tag,Tape,Terms,TicketStatus,TicketStatus_Local,TicketType,TicketType_Local,TicQuantity,Tool_NumberAround,Tool2Descr,Tool3Descr,Tool4Descr,Tool5Descr,ToolNo2,ToolNo3,ToolNo4,ToolNo5,ToolsIn,ToolStat,Total_LineWeight,TotalOrderWeight,TotalShipWeight,TurnBar,UL,updateTimeDateStamp,Use_TurretRewinder,UserDef_MR_1,UserDef_MR_1_Lb,UserDef_MR_2,UserDef_MR_2_Lb'], dtype='object')\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change the column name of 'Number' to 'TicketNumber'\n",
    "tickets_c_m.rename(columns={'Number': 'TicketNumber'}, inplace=True)\n",
    "\n",
    "print('TICKETS CUSTOM MAIN\\n', tickets_c_m.head())\n",
    "print('TICKETS CUSTOM MAIN COLS\\n', tickets_c_m.columns)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('TICKETS STOCK ITEMS\\n', tickets_s_i.head())\n",
    "# print('TICKETS STOCK ITEMS COLS\\n', tickets_s_i.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # change the column name of 'Number' to 'TicketNumber'\n",
    "# tickets_s_m.rename(columns={'Number': 'TicketNumber'}, inplace=True)\n",
    "\n",
    "# print('TICKETS STOCK MAIN\\n', tickets_s_m.head())\n",
    "# print('TICKETS STOCK MAIN COLS\\n', tickets_s_m.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCTS STOCK\n",
      " Adhesive,Alternate,Available,BackOrdered,Box_Size,CaseQty,Color,Commission,Cost,Currency_ExchangeRate,Currency_ID,Customer_Num,CustomerName,Desc1,Desc2,EnteredDate,EnteredTime,eTraxx_Forecast_Quantity,eTraxx_Forecast_Range,FC_Cost,File_Name,ID,Inactive,InternetQuery,Inventory_Expires,Is_LinkedCustomPricesUsed,Is_Location_UsedBy_Consignment,Link_Factor,Location,Material,MaxProduce,MinProduce,ModifiedDate,ModifiedTime,newTimeDateStamp,OnOrder,PackageQty,Part_Type,PhysicalInv,PictDisplayFormat,PK_UUID,PriceMode,PriceMode_Local,ProdClass,ProdSubClass,Product_Image_Size,Product_UniqueProdID,Production_Waste,ProductNo,Releases_Allowed_Default,SupplierName,SupplierNo,SupplierNotes,SupplierPartNo,Tag,TotalCost,UPC,Updated,updateTimeDateStamp,Weight    0\n",
      "dtype: int64\n",
      "----------------------------\n",
      "\n",
      "SALES PRICE INV ADD\n",
      " ID                                 0\n",
      "StockProduct_ID                    0\n",
      "Entered_Date                       0\n",
      "Entered_Time                       0\n",
      "Entered_By                         0\n",
      "Inventory_Date                     0\n",
      "Inventory_Time                     0\n",
      "Inventory_DateTime_Stamp           0\n",
      "Expiration_Date                    0\n",
      "Inventory_Quantity                 0\n",
      "Inventory_Balance                  0\n",
      "Cost_Unit                          0\n",
      "Item_Cost                          0\n",
      "Location                       63975\n",
      "Custom_Ticket_ID                7481\n",
      "Custom_Ticket_CustPONum        10434\n",
      "Source_PONumber                71951\n",
      "Source_POItems_ID              71951\n",
      "Source_Product_UniqueProdID    10553\n",
      "Source_PackSlipItem_ID         10553\n",
      "Notes                           7608\n",
      "Modified_Date                      0\n",
      "Modified_Time                      0\n",
      "Modified_By                    34455\n",
      "Modification_Count                 0\n",
      "Is_User_Made                       0\n",
      "Releases_Allowed                   0\n",
      "PK_UUID                            0\n",
      "Cost_Unit_Local                    0\n",
      "dtype: int64\n",
      "----------------------------\n",
      "\n",
      "SALES PRICE INV REL\n",
      " Cost_Unit                      0\n",
      "Cost_Unit_Local                0\n",
      "Entered_By                     0\n",
      "Entered_Date                   0\n",
      "Entered_Time                   0\n",
      "ID                             0\n",
      "Is_User_Made                   0\n",
      "Item_Cost                      0\n",
      "Kit_Part_Number           206986\n",
      "Kit_Product_ID            206986\n",
      "Location                  125815\n",
      "Modification_Count             0\n",
      "Modified_By               206278\n",
      "Modified_Date                  0\n",
      "Modified_Time                  0\n",
      "Notes                       9807\n",
      "PackSlipItem_ID            18079\n",
      "PK_UUID                        0\n",
      "Release_Date                   0\n",
      "Release_DateTime_Stamp         0\n",
      "Release_Quantity               0\n",
      "Release_Time                   0\n",
      "SP_Inventory_Add_ID            0\n",
      "StockProduct_ID                0\n",
      "StockProduct_Ticket_ID     18079\n",
      "dtype: int64\n",
      "----------------------------\n",
      "\n",
      "TICKETS CUSTOM ITEMS\n",
      " Art                          0\n",
      "Art_Item                     0\n",
      "Art_Ticket                   0\n",
      "Assigned                 67312\n",
      "ColorDescr                   6\n",
      "ConsecNo                 67267\n",
      "CostM                        0\n",
      "Desc2                    67312\n",
      "Description                  7\n",
      "ediLineNumber            67312\n",
      "Equip_NoColors               0\n",
      "Equip_NoFloods               0\n",
      "Equip_Null_Cycles            0\n",
      "Equip3_NoColors              0\n",
      "Equip3_NoFloods              0\n",
      "Equip3_Null_Cycles           0\n",
      "Equip4_NoColors              0\n",
      "Equip4_NoFloods              0\n",
      "Equip4_Null_Cycles           0\n",
      "eTraxx_Customer_Notes    67312\n",
      "FC_LineTotal                 0\n",
      "FC_PriceM                    0\n",
      "ID                           0\n",
      "JobType                      2\n",
      "JobType_Local                2\n",
      "Line_Weight                  0\n",
      "LineTotal                    0\n",
      "Location                 67312\n",
      "MachineCount                 0\n",
      "newTimeDateStamp         38971\n",
      "NoColors                     0\n",
      "NoFloods                     0\n",
      "OrderQuantity                0\n",
      "PK_UUID                      0\n",
      "Plate                        0\n",
      "Plate_Item                   0\n",
      "Plate_Ticket                 0\n",
      "PO_Number                67312\n",
      "Press_Null_Cycles            0\n",
      "PriceM                       0\n",
      "PriceMode                    4\n",
      "PriceMode_Local              4\n",
      "ProdRevisionNo           67306\n",
      "ProductNumber                5\n",
      "Proof                        0\n",
      "Proof_Item                   0\n",
      "Proof_Out                    0\n",
      "Proof_Ticket                 0\n",
      "StckPrdShipStat          67310\n",
      "StockProductID           67308\n",
      "TicketNumber                 0\n",
      "UniquePrice                  0\n",
      "UniqueProdID                 6\n",
      "Unit_Weight                  0\n",
      "updateTimeDateStamp      24036\n",
      "Work_Status              18087\n",
      "dtype: int64\n",
      "----------------------------\n",
      "\n",
      "TICKETS CUSTOM MAIN\n",
      " Act_MakeReady_Footage,Act_OTHER_Hours,ActArtwork,ActFinMaterial,ActFootage,ActMRHrs,ActPackHrs,ActPostPressHours,ActPressSpd,ActQuantity,ActRunHrs,ActStockCost,ActTotalCost,ActualBillings_NetOfSalesTax,ActualCommissionsCost,ActualFanFoldCost,ActualFanfoldHours,ActualFanfoldRate,ActualFootage_StockRolls,ActualGrossMargin_Dollars,ActualGrossMargin_Percent,ActualMSI_StockRolls,ActualNumOfStockRolls,ActualOtherLaborCost,ActualPackagingRate,ActualPackingLaborCost,ActualPostPressLaborCost,ActualPressCost,ActualPressHours,ActualPressRate,ActualRewindingCost,ActualRewindingHours,ActualRewindingRate,ActualTotalFinishing,ActualTotalHours,ActualTotalLaborCosts,ActualTotalMatAndFreightCost,ActualTotalPOCosts,ActWuHrs,AmortizeColorChanges,AmortizePlateChanges,Are_Tools_for_Equip,ArtDone,ArtStat,AutoAppl,BackStage_ColorStrategy,BackStage_DefaultReportForm,BackStage_SmartMarkSet,Bill_Address_ID,Bill_TaxRegion_ID,BillAddr1,BillAddr2,BillCity,BillCountry,BillCounty,BillLocation,BillState,BillZip,Calculation_TimeStamp,CarrierWidth,ColorChangeCost,ColorChangeCost_Extended,ColSpace,ColumnPerf,ConsecNo,CoreSize,CoreType,CornerRadius,CreditHoldOverride,CSA,Currency_ExchangeRate,Currency_ID,Currency_Rate_ID,CustContact_ID,Customer_Total,CustomerName,CustomerNum,CustPONum,DateDone,DateShipped,Due_on_Site_Date,EarlyShipOK,EndUserName,EndUserNum,EndUserPO,EntryBy,EntryDate,EntryTime,Equip_Actual_Cost,Equip_Actual_Hours,Equip_Actual_Length,Equip_Actual_MR_Hours,Equip_Actual_MR_Length,Equip_Actual_Rate,Equip_Actual_Run_Hours,Equip_Actual_Speed,Equip_Actual_WU_Hours,Equip_Done,Equip_EstRunHrs,Equip_EstSpeed,Equip_EstTime,Equip_ID,Equip_MakeReadyHours,Equip_NoAcross,Equip_NoAround,Equip_NumUp_Multiplier,Equip_Status,Equip_WashUpHours,Equip3_Actual_Cost,Equip3_Actual_Hours,Equip3_Actual_Length,Equip3_Actual_MR_Hours,Equip3_Actual_MR_Length,Equip3_Actual_Rate,Equip3_Actual_Run_Hours,Equip3_Actual_Speed,Equip3_Actual_WU_Hours,Equip3_Done,Equip3_EstRunHrs,Equip3_EstSpeed,Equip3_EstTime,Equip3_ID,Equip3_MakeReadyHours,Equip3_NoAcross,Equip3_NoAround,Equip3_NumUp_Multiplier,Equip3_Status,Equip3_WashUpHours,Equip4_Actual_Cost,Equip4_Actual_Hours,Equip4_Actual_Length,Equip4_Actual_MR_Hours,Equip4_Actual_MR_Length,Equip4_Actual_Rate,Equip4_Actual_Run_Hours,Equip4_Actual_Speed,Equip4_Actual_WU_Hours,Equip4_Done,Equip4_EstRunHrs,Equip4_EstSpeed,Equip4_EstTime,Equip4_ID,Equip4_MakeReadyHours,Equip4_NoAcross,Equip4_NoAround,Equip4_NumUp_Multiplier,Equip4_Status,Equip4_WashUpHours,ES_CSR,ES_SalesRep,ESC_Art,ESC_Art_Sales,ESC_Equip,ESC_Equip_Sales,ESC_Equip3,ESC_Equip3_Sales,ESC_Equip4,ESC_Equip4_Sales,ESC_Finish,ESC_Finish_Sales,ESC_Ink,ESC_Ink_Sales,ESC_Plate,ESC_Plate_Sales,ESC_Press,ESC_Press_Sales,ESC_Proof,ESC_Proof_sales,ESC_Ship,ESC_Ship_Sales,ESC_Stock,ESC_Stock_Sales,ESC_TickStat,ESC_TickStat_Sales,ESC_Tool,ESC_Tool_Sales,ESS_Ship,ESS_Ship_Sales,ESS_TickStat,ESS_TickStat_Sales,Est_SetupFootage,Est_SpoilFootage,Est_v_Act_Notes,EstArtwork,EstFinHrs,EstFinMaterial,EstFootage,EstMRHrs,EstPackHrs,EstPostPressHours,EstPressSpd,EstPressTime,EstRunHrs,EstStockCost,EstTime,EstTotal,EstWuHrs,FC_ColorChangeCost,FC_ColorChangeCost_Extended,FC_Customer_Total,FC_EstTotal,FC_MiscCharge,FC_MiscCharge1,FC_MiscCharge2,FC_MiscCharge3,FC_MiscCharge4,FC_PlateChangeCost,FC_PlateChangeCost_Extended,FC_POTotal,FinalUnwind,FinishDone,FinishNotes,FinishStat,FinishType,FlexPack_Gusset,FlexPack_Height,FlexPack_LeftTrim,FlexPack_RightTrim,FlexPack_Type,Frames_Lead_In,Frames_Lead_Out,Freight_AcctNo,GeneralDescr,ID,Image_Rotation,Ink_Status,Internet_Submission,Is_ActBillNetTax_UserModified,Is_AutoOrder,Is_Ink_In,IsPrintReversed,ITSAssocNum,JDF_Note_to_DFE,JDF_Send_Msg,JDF_Sent_On,LabelRepeat,LabelsPer_,LabelsPerFold,LastModified,MainTool,MFGRepName,MFGRepNum,MiscCharge,MiscCharge1,MiscCharge2,MiscCharge3,MiscCharge4,MiscChargeDesc,MiscChargeDesc1,MiscChargeDesc2,MiscChargeDesc3,MiscChargeDesc4,ModifyBy,ModifyDate,ModifyTime,newTimeDateStamp,NoAcross,NoArounPlate,NoColorChanges,NoLabAcrossFin,NoPlateChanges,Number,OnTime,OrderDate,OTSAssocNum,OutsideDiameter,OverRun,Pinfeed,PK_UUID,PlateChangeCost,PlateChangeCost_Extended,PlateDone,PlateStat,POTotal,Press,PressDone,PressStat,PrevJobNum,PriceMode,Priority,ProofDone,ProofStat,RewindEquipNam,RewindEquipNum,RollLength,RollUnit,Roto_CEL_Product_ID,Roto_Quote_Line_ID,Roto_Quote_Number,RowPerf,RowSpace,SalesCommission,Schedule_Status,Screen_Ratio,Shape,Sheet_Height,Sheet_Width,SheetPacktype,Ship_Address_ID,Ship_by_Date,Ship_TaxRegion_ID,ShipAddr1,ShipAddr2,ShipAsOne,ShipAttn_EmailAddress,ShipCity,ShipCountry,ShipCounty,ShipLocation,ShippingInstruc,ShippingStatus,ShipSt,ShipStat,ShipVia,ShipZip,ShrinkSleeve_CutHeight,ShrinkSleeve_LayFlat,ShrinkSleeve_OverLap,SizeAcross,SizeAround,SlitOnRewind,SoldToEndUser,Stock_Allocated,StockDesc1,StockDesc2,StockDesc3,StockIn,StockIn_Local,StockNum1,StockNum2,StockNum3,StockProdDiscnt,StockTicketType,StockWidth1,StockWidth2,StockWidth3,SubTicket,Tab,TabPosition,Tag,Tape,Terms,TicketStatus,TicketStatus_Local,TicketType,TicketType_Local,TicQuantity,Tool_NumberAround,Tool2Descr,Tool3Descr,Tool4Descr,Tool5Descr,ToolNo2,ToolNo3,ToolNo4,ToolNo5,ToolsIn,ToolStat,Total_LineWeight,TotalOrderWeight,TotalShipWeight,TurnBar,UL,updateTimeDateStamp,Use_TurretRewinder,UserDef_MR_1,UserDef_MR_1_Lb,UserDef_MR_2,UserDef_MR_2_Lb    0\n",
      "dtype: int64\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print('PRODUCTS STOCK\\n' ,products_s.isnull().sum())\n",
    "print('----------------------------\\n')\n",
    "print('SALES PRICE INV ADD\\n', sp_inventory_a.isnull().sum())\n",
    "print('----------------------------\\n')\n",
    "print('SALES PRICE INV REL\\n', sp_inventory_r.isnull().sum())\n",
    "print('----------------------------\\n')\n",
    "print('TICKETS CUSTOM ITEMS\\n', tickets_c_i.isnull().sum())\n",
    "print('----------------------------\\n')\n",
    "print('TICKETS CUSTOM MAIN\\n', tickets_c_m.isnull().sum())\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products STOCK - Columns to be dropped: []\n",
      "\n",
      "Products STOCK - Columns to be dropped: []\n",
      "\n",
      "SP Inventory Adds - Columns to be dropped: []\n",
      "\n",
      "SP Inventory Rel - Columns to be dropped: ['Modification_Count']\n",
      "\n",
      "Tickets CUSTOM MAIN - Columns to be dropped: []\n",
      "\n",
      "Tickets CUSTOM MAIN - Columns to be dropped: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns with all rows as missing values (NaN/0)\n",
    "columns_to_drop = products_s.columns[products_s.isnull().all()].tolist()\n",
    "print(f'Products STOCK - Columns to be dropped: {columns_to_drop}\\n')\n",
    "products_s = products_s.dropna(axis=1, how='all')\n",
    "\n",
    "# set the name attribute for each DataFrame\n",
    "products_s.name = 'Products STOCK'\n",
    "sp_inventory_a.name = 'SP Inventory Adds'\n",
    "sp_inventory_r.name = 'SP Inventory Rel'\n",
    "tickets_c_m.name = 'Tickets CUSTOM MAIN'\n",
    "\n",
    "# drop columns with all rows containing the same values of either 0, True, or False\n",
    "for df in [products_s, sp_inventory_a, sp_inventory_r, tickets_c_m]:\n",
    "    columns_to_drop = [col for col in df.columns if df[col].nunique() == 1 and df[col].iloc[0] in [0, True, False]]\n",
    "    print(f'{df.name} - Columns to be dropped: {columns_to_drop}\\n')\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "columns_to_drop = tickets_c_m.columns[tickets_c_m.isnull().all()].tolist()\n",
    "print(f'Tickets CUSTOM MAIN - Columns to be dropped: {columns_to_drop}\\n')\n",
    "tickets_c_m = tickets_c_m.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TICKETS CUSTOM MAIN\\n', tickets_c_m.head())\n",
    "print('TICKETS CUSTOM MAIN COLS\\n', tickets_c_m.columns)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickets CUSTOM ITEMS contains the column 'ProductNumber'\n"
     ]
    }
   ],
   "source": [
    "# check which files have the column 'ProductNumber' in common\n",
    "dataframes = [products_s, sp_inventory_a, sp_inventory_r, tickets_c_i, tickets_c_m]\n",
    "common_column = 'ProductNumber'\n",
    "po_column = 'PONumber'\n",
    "\n",
    "# Set the name attribute for each DataFrame if not already set\n",
    "for df, name in zip(dataframes, ['Products STOCK', 'SP Inventory Adds', 'SP Inventory Rel', 'Tickets CUSTOM ITEMS', 'Tickets CUSTOM MAIN']):\n",
    "\tif not hasattr(df, 'name'):\n",
    "\t\tdf.name = name\n",
    "\n",
    "# Check which dataframes contain the common column\n",
    "for df in dataframes:\n",
    "\tif common_column in df.columns:\n",
    "\t\tprint(f\"{df.name} contains the column '{common_column}'\")\n",
    "\telse:\n",
    "\t\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TicketNumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:153\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:182\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TicketNumber'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m columns_to_display \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicketNumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Find common 'TicketNumber' values\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m common_ticket_numbers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mtickets_c_m\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicketNumber\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mintersection(tickets_c_i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicketNumber\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Filter rows with common 'TicketNumber' values and select specific columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m common_tickets_c_m \u001b[38;5;241m=\u001b[39m tickets_c_m[tickets_c_m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicketNumber\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(common_ticket_numbers)][columns_to_display \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTicQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrderDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShip_by_Date\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3805\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3807\u001b[0m     ):\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TicketNumber'"
     ]
    }
   ],
   "source": [
    "# Specify the columns to display along with 'TicketNumber'\n",
    "columns_to_display = ['TicketNumber']\n",
    "\n",
    "# Find common 'TicketNumber' values\n",
    "common_ticket_numbers = set(tickets_c_m['TicketNumber']).intersection(tickets_c_i['TicketNumber'])\n",
    "\n",
    "# Filter rows with common 'TicketNumber' values and select specific columns\n",
    "common_tickets_c_m = tickets_c_m[tickets_c_m['TicketNumber'].isin(common_ticket_numbers)][columns_to_display + ['TicQuantity', 'OrderDate', 'Ship_by_Date']]\n",
    "common_tickets_c_i = tickets_c_i[tickets_c_i['TicketNumber'].isin(common_ticket_numbers)][columns_to_display + ['OrderQuantity', 'ProductNumber', 'Description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main job description area (summaritive information)\n",
    "print('Common Tickets CUSTOM MAIN\\n', common_tickets_c_m.head())\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job details area (detailed information)\n",
    "print('Common Tickets CUSTOM ITEMS\\n', common_tickets_c_i.head())\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns to display along with 'ProductNumber'\n",
    "columns_to_display = ['ProductNumber']\n",
    "\n",
    "# Find common 'TicketNumber' values\n",
    "common_ticket_numbers = set(products_s['ProductNumber']).intersection(tickets_c_i['ProductNumber'])\n",
    "\n",
    "# Filter rows with common 'TicketNumber' values and select specific columns\n",
    "common_products_s = products_s[products_s['ProductNumber'].isin(common_ticket_numbers)][columns_to_display + ['PhysicalInv', 'Location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# product details related to tickets custom items (link between products and tickets)\n",
    "print('Common Products STOCK\\n', common_products_s.head())\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge common_tickets_c_m and common_tickets_c_i on 'TicketNumber'\n",
    "merged_tickets = pd.merge(common_tickets_c_m, common_tickets_c_i, on='TicketNumber', how='inner')\n",
    "\n",
    "# Ticket MAIN/ITEMS + Products STOCK -> Merged DataFrame\n",
    "# Merge the result with common_products_s on 'ProductNumber'\n",
    "stock_forecasting_merge_df = pd.merge(merged_tickets, common_products_s, on='ProductNumber', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "REASON FOR 'TicQuantity' AND 'OrderQuantity' DIFFERENCE:\n",
    "\n",
    "Ticket Quantity (TicQuantity) [MAIN]: Amount of labels ordered by the customer (multiple different products can be ordered in one ticket)\n",
    "\n",
    "Order Quantity (OrderQuantity) [ITEMS]: Amount of the same product ordered by the customer (one product)\n",
    "'''\n",
    "\n",
    "# check if all 'TicQuantity' values are the same as 'OrderQuantity', output boolean\n",
    "print('TicQuantity == OrderQuantity\\n', stock_forecasting_merge_df['TicQuantity'].eq(stock_forecasting_merge_df['OrderQuantity']).all())\n",
    "print('----------------------------\\n')\n",
    "\n",
    "# output all 'orderQuantity' values from highest to lowest\n",
    "print('OrderQuantity\\n', stock_forecasting_merge_df['OrderQuantity'].sort_values(ascending=False).head(20))\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'TicQuantity' is good for total ticket quantity, 'OrderQuantity' is good for individual product quantity\n",
    "\n",
    "Dropping this keeps the project from being confusing. It works without this column since there are multiple 'TicketNumber' values with the item specific 'OrderQuantity'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_forecasting_merge_df = stock_forecasting_merge_df.drop(columns=['TicQuantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Merge datasets\n",
    "merged_data = pd.merge(tickets_c_m, tickets_c_i, on='TicketNumber')\n",
    "merged_data = pd.merge(merged_data, products_s, on='ProductNumber')\n",
    "\n",
    "# Feature engineering\n",
    "merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'])\n",
    "merged_data['Ship_by_Date'] = pd.to_datetime(merged_data['Ship_by_Date'])\n",
    "merged_data['order_month'] = merged_data['OrderDate'].dt.month\n",
    "merged_data['order_week'] = merged_data['OrderDate'].dt.isocalendar().week\n",
    "\n",
    "# Aggregate data\n",
    "product_sales = merged_data.groupby(['ProductNumber', 'order_month']).agg({'OrderQuantity': 'sum'}).reset_index()\n",
    "\n",
    "# Encode 'ProductNumber' as numeric\n",
    "label_encoder = LabelEncoder()\n",
    "product_sales['ProductNumber'] = label_encoder.fit_transform(product_sales['ProductNumber'])\n",
    "\n",
    "# Define features and target\n",
    "X = product_sales[['ProductNumber', 'order_month']]\n",
    "y = product_sales['OrderQuantity']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the structure of the merged DataFrame\n",
    "stock_forecasting_merge_df = stock_forecasting_merge_df[\n",
    "    ['TicketNumber',\n",
    "     'OrderDate', \n",
    "     'Ship_by_Date',\n",
    "     'PhysicalInv',\n",
    "     'OrderQuantity', \n",
    "     'ProductNumber', \n",
    "     'Description',\n",
    "     'Location']]\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print('Final Merged DataFrame\\n', stock_forecasting_merge_df.head(60))\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all rows with values of 'PhysicalInv' as 0 into one DataFrame\n",
    "zero_physical_inv = stock_forecasting_merge_df[stock_forecasting_merge_df['PhysicalInv'] == 0]\n",
    "\n",
    "# save all rows with values of 'PhysicalInv' > 0 into one DataFrame\n",
    "positive_physical_inv = stock_forecasting_merge_df[stock_forecasting_merge_df['PhysicalInv'] > 0]\n",
    "\n",
    "print('Zero Physical Inventory\\n', zero_physical_inv.head())\n",
    "print('----------------------------\\n')\n",
    "\n",
    "print('Positive Physical Inventory\\n', positive_physical_inv.head())\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if zero_physical_inv has 'OrderQuantity' any values of 0\n",
    "print('OrderQuantity == 0\\n', zero_physical_inv['OrderQuantity'].eq(0).any())\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the amount of rows in each DataFrame\n",
    "print('Zero Physical Inventory\\n', zero_physical_inv.shape)\n",
    "print('----------------------------\\n')\n",
    "\n",
    "# drop values that are both order quantity and physical inventory are 0\n",
    "zero_physical_inv = zero_physical_inv[~((zero_physical_inv['PhysicalInv'] == 0) & (zero_physical_inv['OrderQuantity'] == 0))]\n",
    "\n",
    "# check if zero_physical_inv has 'OrderQuantity' any values of 0\n",
    "print('OrderQuantity == 0\\n', zero_physical_inv['OrderQuantity'].eq(0).any())\n",
    "print('----------------------------\\n')\n",
    "\n",
    "# print the amount of rows in each DataFrame\n",
    "print('Zero Physical Inventory\\n', zero_physical_inv.shape)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first rows of zero_physical_inv\n",
    "print('Zero Physical Inventory\\n', zero_physical_inv.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first rows of positive_physical_inv\n",
    "print('Positive Physical Inventory\\n', positive_physical_inv.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the amount of rows in each DataFrame\n",
    "print('Positive Physical Inventory\\n', positive_physical_inv.shape)\n",
    "print('----------------------------\\n')\n",
    "\n",
    "positive_physical_inv = positive_physical_inv[positive_physical_inv['OrderQuantity'] > 0]\n",
    "\n",
    "# check if positive_physical_inv has 'OrderQuantity' any values of 0\n",
    "print('OrderQuantity == 0\\n', positive_physical_inv['OrderQuantity'].eq(0).any())\n",
    "print('----------------------------\\n')\n",
    "\n",
    "# print the amount of rows in each DataFrame\n",
    "print('Positive Physical Inventory\\n', positive_physical_inv.shape)\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first rows of positive_physical_inv\n",
    "print('Positive Physical Inventory\\n', positive_physical_inv.head(20))\n",
    "print('----------------------------\\n')\n",
    "\n",
    "# display the first rows of zero_physical_inv\n",
    "print('Zero Physical Inventory\\n', zero_physical_inv.head(20))\n",
    "print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a method to find the best possible hyperparameters for the model\n",
    "def find_best_hyperparameters(model, parameter_grid, X_train, y_train):\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=parameter_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f'{model.__class__.__name__} Best Parameters: {grid_search.best_params_}')    \n",
    "\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    print('Evaluating model...')\n",
    "    \n",
    "    # Output type of model\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "\n",
    "    # K-fold cross validation (k=5)\n",
    "    y_pred = cross_val_predict(model, X, y, cv=5)\n",
    "\n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "    # R-squared (R)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    print(f\"R-squared (R): {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'ProductNumber' and create separate arrays for 'OrderQuantity'\n",
    "product_groups = stock_forecasting_merge_df.groupby('ProductNumber')['OrderQuantity'].apply(list)\n",
    "\n",
    "# Convert the grouped data into a dictionary of arrays\n",
    "product_order_quantities = {product: np.array(quantities) for product, quantities in product_groups.items()}\n",
    "\n",
    "# Filter the dictionary to only include product numbers with at least one order quantity over 300,000 (edit the threshold as needed to find out for feature engineering)\n",
    "filtered_product_order_quantities = {product: quantities for product, quantities in product_order_quantities.items() if any(quantities > 300000)}\n",
    "\n",
    "# Display the filtered arrays\n",
    "print('Filtered Product Order Quantities (300k for one Order Quantity)\\n')\n",
    "for product, quantities in filtered_product_order_quantities.items():\n",
    "    print(f'Product: {product}, Order Quantities: {quantities}')\n",
    "    print('----------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ensure 'OrderDate' and 'Ship_by_Date' are in datetime format\n",
    "stock_forecasting_merge_df['OrderDate'] = pd.to_datetime(stock_forecasting_merge_df['OrderDate'])\n",
    "\n",
    "# Handle invalid date values in 'Ship_by_Date'\n",
    "stock_forecasting_merge_df['Ship_by_Date'] = pd.to_datetime(\n",
    "    stock_forecasting_merge_df['Ship_by_Date'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Calculate 'Lead_Time' in days\n",
    "stock_forecasting_merge_df['Lead_Time'] = (stock_forecasting_merge_df['Ship_by_Date'] - stock_forecasting_merge_df['OrderDate']).dt.days\n",
    "# Handle missing values in 'Lead_Time'\n",
    "stock_forecasting_merge_df['Lead_Time'].fillna(stock_forecasting_merge_df['Lead_Time'].median(), inplace=True)\n",
    "\n",
    "# Convert 'OrderQuantity' to numeric, forcing errors to NaN and then filling them with 0\n",
    "stock_forecasting_merge_df['OrderQuantity'] = pd.to_numeric(stock_forecasting_merge_df['OrderQuantity'], errors='coerce').fillna(0)\n",
    "\n",
    "# define the features and target variable\n",
    "X = stock_forecasting_merge_df[['PhysicalInv', 'Lead_Time']]\n",
    "y = stock_forecasting_merge_df['OrderQuantity']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split with scaled features\n",
    "X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make model using Positive PhysicalInv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'OrderDate' and 'Ship_by_Date' are in datetime format\n",
    "positive_physical_inv['OrderDate'] = pd.to_datetime(positive_physical_inv['OrderDate'])\n",
    "\n",
    "# Handle invalid date values in 'Ship_by_Date'\n",
    "positive_physical_inv['Ship_by_Date'] = pd.to_datetime(\n",
    "    positive_physical_inv['Ship_by_Date'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Calculate 'Lead_Time' in days\n",
    "positive_physical_inv['Lead_Time'] = (positive_physical_inv['Ship_by_Date'] - positive_physical_inv['OrderDate']).dt.days\n",
    "# Handle missing values in 'Lead_Time'\n",
    "positive_physical_inv['Lead_Time'].fillna(positive_physical_inv['Lead_Time'].median(), inplace=True)\n",
    "\n",
    "# Convert 'OrderQuantity' to numeric, forcing errors to NaN and then filling them with 0\n",
    "positive_physical_inv['OrderQuantity'] = pd.to_numeric(positive_physical_inv['OrderQuantity'], errors='coerce').fillna(0)\n",
    "\n",
    "# define the features and target variable\n",
    "X = positive_physical_inv[['PhysicalInv', 'Lead_Time']]\n",
    "y = positive_physical_inv['OrderQuantity']\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split with scaled features\n",
    "X_train_scaled, X_val_scaled, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------------------------------------------------------\n",
    "REGRESSION MODEL PARAM GRIDS\n",
    "--------------------------------------------------------\n",
    "'''\n",
    "rf_param_grid = { # Random Forest Regressor\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['squared_error'], # tested other values, but this was the best one\n",
    "    'n_jobs': [1, 2, 4],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "lr_param_grid = { # Linear Regression\n",
    "    'fit_intercept': [True, False],\n",
    "    'positive': [True, False],\n",
    "    'n_jobs': [1, 2, 4],\n",
    "    'copy_X': [True, False]\n",
    "}\n",
    "\n",
    "svr_param_grid = { # Support Vector Regressor\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'], # 'poly' takes long to compute\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "dt_param_grid = { # Decision Tree Regressor\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "nn_param_grid = { # Neural Network Regressor\n",
    "    'model__epochs': [50, 100],\n",
    "    'model__batch_size': [32, 64],\n",
    "}\n",
    "\n",
    "'''\n",
    "--------------------------------------------------------\n",
    "CLASSIFICATION MODELS\n",
    "--------------------------------------------------------\n",
    "'''\n",
    "\n",
    "rf_class_param_grid = { # Random Forest Classifier\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini'],\n",
    "    'n_jobs': [1, 2, 4],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "lr_class_param_grid = { # Logistic Regression\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'n_jobs': [1, 2, 4]\n",
    "}\n",
    "\n",
    "svc_class_param_grid = { # Support Vector Classifier\n",
    "    'kernel': ['rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "}\n",
    "\n",
    "dt_class_param_grid = { # Decision Tree Classifier\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'random_state': [42],\n",
    "    'splitter': ['best', 'random']\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from seaborn import heatmap\n",
    "\n",
    "# Define the features and target variable for classification\n",
    "X_class = stock_forecasting_merge_df[['PhysicalInv', 'Lead_Time']]\n",
    "y_class = stock_forecasting_merge_df['OrderQuantity'].apply(lambda x: 1 if x > 0 else 0)  # Binary classification: 1 if OrderQuantity > 0, else 0\n",
    "\n",
    "# Train-test split for classification\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classification model (Random Forest Classifier) with param grid\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_class_params = find_best_hyperparameters(rf_classifier, parameter_grid=rf_class_param_grid, X_train=X_train_class, y_train=y_train_class)\n",
    "\n",
    "# Train the classification model with the best hyperparameters\n",
    "rf_classifier = RandomForestClassifier(**rf_class_params)\n",
    "rf_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predict using the classification model\n",
    "y_pred_class = rf_classifier.predict(X_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the classification model (Logistic Regression) with param grid\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_class_params = find_best_hyperparameters(lr_classifier, parameter_grid=lr_class_param_grid, X_train=X_train_class, y_train=y_train_class)\n",
    "\n",
    "# Train the classification model with the best hyperparameters\n",
    "lr_classifier = LogisticRegression(**lr_class_params)\n",
    "lr_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predict using the classification model\n",
    "y_pred_class = lr_classifier.predict(X_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train the classification model (Support Vector Classifier) with param grid\n",
    "svc_classifier = SVC()\n",
    "svc_class_params = find_best_hyperparameters(svc_classifier, parameter_grid=svc_class_param_grid, X_train=X_train_class, y_train=y_train_class)\n",
    "\n",
    "# Train the classification model with the best hyperparameters\n",
    "svc_classifier = SVC(**svc_class_params)\n",
    "svc_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predict using the classification model\n",
    "y_pred_class = svc_classifier.predict(X_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Train the classification model (Decision Tree Classifier) with param grid\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "dt_class_params = find_best_hyperparameters(dt_classifier, parameter_grid=dt_class_param_grid, X_train=X_train_class, y_train=y_train_class)\n",
    "\n",
    "# Train the classification model with the best hyperparameters\n",
    "dt_classifier = DecisionTreeClassifier(**dt_class_params)\n",
    "dt_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Predict using the classification model\n",
    "y_pred_class = dt_classifier.predict(X_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_class, y_pred_class)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display the classification report\n",
    "class_report = classification_report(y_test_class, y_pred_class)\n",
    "print('Classification Report\\n', class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training - Takes about 40 minutes for all models (try not to keep running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train the model with scaled features (Random Forest Regressor)\n",
    "rf = RandomForestRegressor()\n",
    "rf_params = find_best_hyperparameters(rf, rf_param_grid, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(**rf_params)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with scaled features (Linear Regression)\n",
    "lr = LinearRegression()\n",
    "lr_params = find_best_hyperparameters(lr, lr_param_grid, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(**lr_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with scaled features (Support Vector Regressor)\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr_params = find_best_hyperparameters(svr, svr_param_grid, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(**svr_params)\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "y_pred_svr = svr.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with scaled features (Decision Tree Regressor)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor()\n",
    "dt_params = find_best_hyperparameters(dt, dt_param_grid, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeRegressor(**dt_params)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NN METRICS ARE NOT ARENT BETTER THAN BASE MODELS, SO ITS COMMENTED OUT\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from scikeras.wrappers import KerasRegressor\n",
    "# from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# # Define the neural network model\n",
    "# def create_nn_model(input_shape):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(64, input_dim=input_shape, activation='relu'))\n",
    "#     model.add(Dense(32, activation='relu'))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "#     return model\n",
    "\n",
    "# # Wrap the Keras model with KerasRegressor\n",
    "# input_shape = X_train_scaled.shape[1]\n",
    "# nn_model = KerasRegressor(build_fn=create_nn_model, input_shape=input_shape, verbose=1)\n",
    "\n",
    "# # Define the parameter grid for GridSearchCV\n",
    "# nn_param_grid = {\n",
    "#     'epochs': [50, 100],\n",
    "#     'batch_size': [32, 64]\n",
    "# }\n",
    "\n",
    "# # Perform GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=nn_model, param_grid=nn_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# # Get the best parameters\n",
    "# nn_params = grid_search.best_params_\n",
    "# print(f'Best Parameters: {nn_params}')\n",
    "\n",
    "# # Set the best parameters to the model\n",
    "# nn_model.set_params(**nn_params)\n",
    "\n",
    "# # Use cross_val_score or other scikit-learn utilities\n",
    "# scores = cross_val_score(nn_model, X_train_scaled, y_train, cv=5)\n",
    "# print(scores)\n",
    "\n",
    "# # Train the model\n",
    "# history = nn_model.fit(X_train_scaled, y_train, epochs=nn_params['epochs'], batch_size=nn_params['batch_size'], validation_split=0.2, verbose=1)\n",
    "\n",
    "# # Predict using the neural network model\n",
    "# y_pred_nn = nn_model.predict(X_val_scaled)\n",
    "\n",
    "# # Evaluate the model\n",
    "# evaluate_model(nn_model, X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM model COMMENTED OUT DUE TO METRICS NOT BEING IMPROVED\n",
    "\n",
    "# from keras.layers import LSTM, Dropout\n",
    "# from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# # Define the LSTM model\n",
    "# def create_lstm_model(input_shape):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(50, activation='relu', input_shape=input_shape))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(1))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "#     return model\n",
    "\n",
    "# # Prepare the data for LSTM\n",
    "# # Reshape the data to be 3D [samples, timesteps, features]\n",
    "# X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# # Split the reshaped data into training and validation sets\n",
    "# X_train_reshaped, X_val_reshaped, y_train, y_val = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create the LSTM model\n",
    "# input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])\n",
    "# lstm_model = create_lstm_model(input_shape)\n",
    "\n",
    "# # Train the LSTM model\n",
    "# history = lstm_model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_data=(X_val_reshaped, y_val), verbose=1)\n",
    "\n",
    "# # Predict using the LSTM model\n",
    "# y_pred_lstm = lstm_model.predict(X_val_reshaped)\n",
    "\n",
    "# # Evaluate the LSTM model\n",
    "# evaluate_model(lstm_model, X_reshaped, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf, X_scaled, y)\n",
    "print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lr, X_scaled, y)\n",
    "print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(svr, X_scaled, y)\n",
    "print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt, X_scaled, y)\n",
    "print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with CV & Outliter removal\n",
    "| Evaluation Metric | Model |   \n",
    "| --- | --- |\n",
    "| MAE | 84860.9381 |\n",
    "| MSE | 41863556821.9018 |\n",
    "| RMSE | 204605.8573 |\n",
    "| R2 | 0.1196 |\n",
    "\n",
    "Random Forest with CV & Param grid (With outliers)\n",
    "| Evaluation Metric | Model |   \n",
    "| --- | --- |\n",
    "| MAE | 81106.3853 |\n",
    "| MSE | 29933938412.4621 |\n",
    "| RMSE | 173014.2723 |\n",
    "| R2 | 0.2059 |\n",
    "\n",
    "Random Forest - CV & Outlier removal & Param grid (Doesnt look correct on plot)\n",
    "| Evaluation Metric | Model |\n",
    "| --- | --- |\n",
    "| MAE | 7.2066 |\n",
    "| MSE | 758.3109 |\n",
    "| RMSE | 27.5374 |\n",
    "| R2 | -0.1655 |\n",
    "\n",
    "Random Forest - PhysicalInv > 0 (CV & Paramgrid)\n",
    "\n",
    "| Evaluation Metric | Model |\n",
    "| --- | --- |\n",
    "| MAE | 104017.0708 |\n",
    "| MSE | 47589838444.2892 |\n",
    "| RMSE | 218150.9533 |\n",
    "| R2 | 0.3368 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ALL ENSEMBLE METHODS ARE COMMENTED OUT BECAUSE METRICS ARE NOT IMPROVED\n",
    "\n",
    "# # implement ensemble methods (bagging, boosting, stacking) to improve the model\n",
    "# from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, StackingRegressor\n",
    "# from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# print('--------------------- Bagging Regressor -----------------------')\n",
    "# # Bagging Regressor\n",
    "# bagging = BaggingRegressor(estimator=RandomForestRegressor(), n_estimators=10, random_state=42)\n",
    "# bagging.fit(X_train_scaled, y_train)\n",
    "# y_pred_bagging = bagging.predict(X_val_scaled)\n",
    "# evaluate_model(bagging, X_scaled, y)\n",
    "\n",
    "# print('---------------------- AdaBoost Regressor ----------------------')\n",
    "# # AdaBoost Regressor\n",
    "# adaboost = AdaBoostRegressor(estimator=RandomForestRegressor(), n_estimators=50, random_state=42)\n",
    "# adaboost.fit(X_train_scaled, y_train)\n",
    "# y_pred_adaboost = adaboost.predict(X_val_scaled)\n",
    "# evaluate_model(adaboost, X_scaled, y)\n",
    "\n",
    "# print('---------------------- Stacking Regressor ----------------------')\n",
    "# # Stacking Regressor\n",
    "# estimators = [\n",
    "#     ('rf', RandomForestRegressor()),\n",
    "#     ('lr', LinearRegression()),\n",
    "#     ('svr', SVR())\n",
    "# ]\n",
    "\n",
    "# stacking = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\n",
    "# stacking.fit(X_train_scaled, y_train)\n",
    "# y_pred_stacking = stacking.predict(X_val_scaled)\n",
    "# evaluate_model(stacking, X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\n",
    "\n",
    "# Plot the bar plot to compare the actual and predicted values\n",
    "axes[0].bar(positive_physical_inv.loc[y_val.index, 'ProductNumber'], y_val, label='Actual', alpha=0.6)\n",
    "axes[0].bar(positive_physical_inv.loc[y_val.index, 'ProductNumber'], y_pred_rf, label='Predicted', alpha=0.6)\n",
    "axes[0].set_xlabel('Product Number')\n",
    "axes[0].set_ylabel('Order Quantity')\n",
    "axes[0].set_title('Actual vs Predicted Order Quantity')\n",
    "axes[0].set_ylim(0, 1000000)\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot the residual plot\n",
    "residuals = y_val - y_pred_rf\n",
    "axes[1].scatter(positive_physical_inv.loc[y_val.index, 'ProductNumber'], residuals, alpha=0.6)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Product Number')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Residual Plot')\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
