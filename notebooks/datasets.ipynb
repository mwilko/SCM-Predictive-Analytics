{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset initialization and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/2542517297.py:4: DtypeWarning: Columns (4,6,20,50,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  products_s = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] Products [STOCK].txt', sep='\\t', header=0) # stock\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/2542517297.py:5: DtypeWarning: Columns (5,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tickets_c_i = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] Tickets [CUSTOM] [ITEMS].txt', sep='\\t', header=0) # customer order items\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/2542517297.py:6: DtypeWarning: Columns (43,57,104,139,210,244,251,289,293,310,321,324,326,344,348) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tickets_c_m = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] Tickets [CUSTOM] [MAIN].txt', sep='\\t', header=0) # customer order main data\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/2542517297.py:7: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  sp_inv_adds = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] SP Inventory [ADDS].txt', sep='\\t', header=0) # customer order main data\n"
     ]
    }
   ],
   "source": [
    "# convert datasets to csv from txt\n",
    "\n",
    "# \\t used as separator, because of raw data format and the headers as row 0\n",
    "products_s = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] Products [STOCK].txt', sep='\\t', header=0) # stock\n",
    "tickets_c_i = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] Tickets [CUSTOM] [ITEMS].txt', sep='\\t', header=0) # customer order items\n",
    "tickets_c_m = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] Tickets [CUSTOM] [MAIN].txt', sep='\\t', header=0) # customer order main data\n",
    "sp_inv_adds = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] SP Inventory [ADDS].txt', sep='\\t', header=0) # customer order main data\n",
    "sp_inv_rel = pd.read_csv('../datasets/stock_forecasting/raw/2022-2025/[LT] SP Inventory [REL].txt', sep='\\t', header=0) # customer order main data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Product [STOCK] ----------------\n",
      "   Adhesive  Alternate  Available  BackOrdered Box_Size  CaseQty Color  \\\n",
      "0       NaN        NaN          0            0      NaN      NaN   NaN   \n",
      "1       NaN        NaN          0            0      NaN      NaN   NaN   \n",
      "2       NaN        NaN          0            0      NaN      NaN   NaN   \n",
      "3       NaN        NaN          0            0      NaN      NaN   NaN   \n",
      "4       NaN        NaN          0            0      NaN      NaN   NaN   \n",
      "\n",
      "   Commission   Cost  Currency_ExchangeRate  ...  SupplierName  SupplierNo  \\\n",
      "0           0   0.00                      0  ...           NaN         NaN   \n",
      "1           0   0.00                      0  ...           NaN         NaN   \n",
      "2           0   0.00                      0  ...           NaN         NaN   \n",
      "3           0   0.00                      0  ...           NaN         NaN   \n",
      "4           0  49.11                      0  ...           NaN         NaN   \n",
      "\n",
      "  SupplierNotes SupplierPartNo Tag TotalCost UPC   Updated  \\\n",
      "0           NaN            NaN NaN         0 NaN  00/00/00   \n",
      "1           NaN            NaN NaN         0 NaN  00/00/00   \n",
      "2           NaN            NaN NaN         0 NaN  00/00/00   \n",
      "3           NaN            NaN NaN         0 NaN  00/00/00   \n",
      "4           NaN            NaN NaN         0 NaN  00/00/00   \n",
      "\n",
      "        updateTimeDateStamp  Weight  \n",
      "0  2024-11-12T18:44:36.831Z       0  \n",
      "1  2024-11-12T18:59:23.570Z       0  \n",
      "2  2024-11-12T18:44:36.849Z       0  \n",
      "3  2024-11-12T18:44:36.879Z       0  \n",
      "4  2024-11-12T18:48:39.525Z       0  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "-------------- Tickets [CUSTOM] [ITEMS] ----------------\n",
      "     Art  Art_Item  Art_Ticket  Assigned                        ColorDescr  \\\n",
      "0  False     False       False       NaN         C: WHITE, CMYOK, LAM ADH    \n",
      "1  False     False       False       NaN              C: MYK, GLOSS 54121    \n",
      "2  False     False       False       NaN  C: CMYK,7512C,363C,286C,LAM ADH    \n",
      "3  False     False       False       NaN  C: CMYK,7512C,363C,286C,LAM ADH    \n",
      "4  False     False       False       NaN                C: CMYK, GLOSS V     \n",
      "\n",
      "  ConsecNo  CostM  Desc2                                   Description  \\\n",
      "0      NaN      0    NaN            510016W Gumout All In One FSC 10oz   \n",
      "1      NaN      0    NaN       510022W Gumout Octane Booster 10oz Wrap   \n",
      "2      NaN      0    NaN   Simply Nature 93/7 Ground Beef Famil Pack B   \n",
      "3      NaN      0    NaN  Simply Nature 93/7 Ground Beef Family Pack F   \n",
      "4      NaN      0    NaN      72969 FL Ginger Sweet Chili Sauce 14.5oz   \n",
      "\n",
      "   ediLineNumber  ...  Proof_Out  Proof_Ticket  StckPrdShipStat  \\\n",
      "0            NaN  ...   00/00/00         False              NaN   \n",
      "1            NaN  ...   00/00/00         False              NaN   \n",
      "2            NaN  ...   00/00/00         False              NaN   \n",
      "3            NaN  ...   00/00/00         False              NaN   \n",
      "4            NaN  ...   00/00/00         False              NaN   \n",
      "\n",
      "   StockProductID  TicketNumber  UniquePrice  UniqueProdID  Unit_Weight  \\\n",
      "0             NaN         99999        False       11704.0            0   \n",
      "1             NaN         99997        False        7728.0            0   \n",
      "2             NaN         99996        False       28747.0            0   \n",
      "3             NaN         99995        False       28746.0            0   \n",
      "4             NaN         99993        False       10474.0            0   \n",
      "\n",
      "        updateTimeDateStamp   Work_Status  \n",
      "0  2024-11-07T15:10:02.804Z           NaN  \n",
      "1  2024-11-07T15:03:26.338Z           NaN  \n",
      "2  2024-11-28T15:28:19.649Z  8. In Plates  \n",
      "3  2024-11-13T17:45:49.909Z  8. In Plates  \n",
      "4  2024-11-07T14:38:30.627Z           NaN  \n",
      "\n",
      "[5 rows x 56 columns]\n",
      "-------------- Tickets [CUSTOM] [MAIN] ----------------\n",
      "   Act_MakeReady_Footage  Act_OTHER_Hours  ActArtwork  ActFinMaterial  \\\n",
      "0                      0                0           0               0   \n",
      "1                      0                0           0               0   \n",
      "2                      0                0           0               0   \n",
      "3                      0                0           0               0   \n",
      "4                      0                0           0               0   \n",
      "\n",
      "   ActFootage  ActMRHrs  ActPackHrs  ActPostPressHours  ActPressSpd  \\\n",
      "0           0       0.0           0                  0            0   \n",
      "1           0       0.0           0                  0            0   \n",
      "2           0       0.0           0                  0            0   \n",
      "3           0       0.0           0                  0            0   \n",
      "4           0       0.0           0                  0            0   \n",
      "\n",
      "   ActQuantity  ...  TotalOrderWeight  TotalShipWeight  TurnBar     UL  \\\n",
      "0            0  ...                 0                0    False  False   \n",
      "1            0  ...                 0                0    False  False   \n",
      "2            0  ...                 0                0    False  False   \n",
      "3            0  ...                 0                0    False  False   \n",
      "4            0  ...                 0                0    False  False   \n",
      "\n",
      "        updateTimeDateStamp  Use_TurretRewinder  UserDef_MR_1  \\\n",
      "0  2025-01-17T15:28:17.611Z               False         False   \n",
      "1  2025-01-17T15:14:34.082Z               False         False   \n",
      "2  2025-01-17T15:01:30.497Z               False         False   \n",
      "3  2025-01-17T15:00:29.967Z               False         False   \n",
      "4  2025-01-17T14:59:59.708Z               False         False   \n",
      "\n",
      "   UserDef_MR_1_Lb  UserDef_MR_2  UserDef_MR_2_Lb  \n",
      "0              NaN         False              NaN  \n",
      "1              NaN         False              NaN  \n",
      "2              NaN         False              NaN  \n",
      "3              NaN         False              NaN  \n",
      "4              NaN         False              NaN  \n",
      "\n",
      "[5 rows x 365 columns]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# display the first 5 rows of the datasets\n",
    "print('-------------- Product [STOCK] ----------------')\n",
    "print(products_s.head())\n",
    "\n",
    "print('-------------- Tickets [CUSTOM] [ITEMS] ----------------')\n",
    "print(tickets_c_i.head())\n",
    "\n",
    "print('-------------- Tickets [CUSTOM] [MAIN] ----------------')\n",
    "print(tickets_c_m.head())\n",
    "print('------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change product stock column 'ProductNo' to 'ProductNumber' to match the other datasets\n",
    "products_s.rename(columns={'ProductNo': 'ProductNumber'}, inplace=True)\n",
    "\n",
    "# change the column name of 'Number' to 'TicketNumber'\n",
    "tickets_c_m.rename(columns={'Number': 'TicketNumber'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Act_MakeReady_Footage</th>\n",
       "      <th>Act_OTHER_Hours</th>\n",
       "      <th>ActArtwork</th>\n",
       "      <th>ActFinMaterial</th>\n",
       "      <th>ActFootage</th>\n",
       "      <th>ActMRHrs</th>\n",
       "      <th>ActPackHrs</th>\n",
       "      <th>ActPostPressHours</th>\n",
       "      <th>ActPressSpd</th>\n",
       "      <th>ActQuantity</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalOrderWeight</th>\n",
       "      <th>TotalShipWeight</th>\n",
       "      <th>TurnBar</th>\n",
       "      <th>UL</th>\n",
       "      <th>updateTimeDateStamp</th>\n",
       "      <th>Use_TurretRewinder</th>\n",
       "      <th>UserDef_MR_1</th>\n",
       "      <th>UserDef_MR_1_Lb</th>\n",
       "      <th>UserDef_MR_2</th>\n",
       "      <th>UserDef_MR_2_Lb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-17T15:28:17.611Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-17T15:14:34.082Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-17T15:01:30.497Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-17T15:00:29.967Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-17T14:59:59.708Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 365 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Act_MakeReady_Footage  Act_OTHER_Hours  ActArtwork  ActFinMaterial  \\\n",
       "0                      0                0           0               0   \n",
       "1                      0                0           0               0   \n",
       "2                      0                0           0               0   \n",
       "3                      0                0           0               0   \n",
       "4                      0                0           0               0   \n",
       "\n",
       "   ActFootage  ActMRHrs  ActPackHrs  ActPostPressHours  ActPressSpd  \\\n",
       "0           0       0.0           0                  0            0   \n",
       "1           0       0.0           0                  0            0   \n",
       "2           0       0.0           0                  0            0   \n",
       "3           0       0.0           0                  0            0   \n",
       "4           0       0.0           0                  0            0   \n",
       "\n",
       "   ActQuantity  ...  TotalOrderWeight  TotalShipWeight  TurnBar     UL  \\\n",
       "0            0  ...                 0                0    False  False   \n",
       "1            0  ...                 0                0    False  False   \n",
       "2            0  ...                 0                0    False  False   \n",
       "3            0  ...                 0                0    False  False   \n",
       "4            0  ...                 0                0    False  False   \n",
       "\n",
       "        updateTimeDateStamp  Use_TurretRewinder  UserDef_MR_1  \\\n",
       "0  2025-01-17T15:28:17.611Z               False         False   \n",
       "1  2025-01-17T15:14:34.082Z               False         False   \n",
       "2  2025-01-17T15:01:30.497Z               False         False   \n",
       "3  2025-01-17T15:00:29.967Z               False         False   \n",
       "4  2025-01-17T14:59:59.708Z               False         False   \n",
       "\n",
       "   UserDef_MR_1_Lb  UserDef_MR_2  UserDef_MR_2_Lb  \n",
       "0              NaN         False              NaN  \n",
       "1              NaN         False              NaN  \n",
       "2              NaN         False              NaN  \n",
       "3              NaN         False              NaN  \n",
       "4              NaN         False              NaN  \n",
       "\n",
       "[5 rows x 365 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickets_c_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the datasets\n",
    "merged_data = pd.merge(tickets_c_m, tickets_c_i, on='TicketNumber')\n",
    "merged_data = pd.merge(merged_data, products_s, on='ProductNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- DROP BOOLEAN AND NULL COLUMNS (NOT NEEDED) -----\n",
      "\n",
      "Merged Data - Columns to be dropped (Missing Vals): ['BackStage_DefaultReportForm', 'BackStage_SmartMarkSet', 'BillCounty', 'CoreType', 'EndUserNum', 'EndUserPO', 'Equip3_ID', 'Equip3_Status', 'Equip4_Status', 'Est_v_Act_Notes', 'Freight_AcctNo', 'Ink_Status', 'JDF_Note_to_DFE', 'JDF_Send_Msg', 'MiscChargeDesc1', 'MiscChargeDesc2', 'MiscChargeDesc3', 'MiscChargeDesc4', 'PlateStat', 'ProofStat', 'Roto_CEL_Product_ID', 'Roto_Quote_Line_ID', 'Roto_Quote_Number', 'Schedule_Status', 'ShipAttn_EmailAddress', 'ShipCounty', 'ShipStat', 'Tag_x', 'Tool3Descr', 'Tool4Descr', 'Tool5Descr', 'ToolNo3', 'ToolNo4', 'ToolNo5', 'ToolStat', 'UserDef_MR_1_Lb', 'UserDef_MR_2_Lb', 'Assigned', 'Desc2_x', 'ediLineNumber', 'eTraxx_Customer_Notes', 'Location_x', 'PO_Number', 'Adhesive', 'Alternate', 'Box_Size', 'CaseQty', 'Color', 'eTraxx_Forecast_Range', 'File_Name', 'Material', 'SupplierNotes', 'Tag_y', 'UPC']\n",
      "\n",
      "Merged Data - Columns to be dropped (All cols with rows same vals): ['Act_OTHER_Hours', 'ActArtwork', 'ActFinMaterial', 'ActPostPressHours', 'ActualBillings_NetOfSalesTax', 'ActualCommissionsCost', 'ActualFanFoldCost', 'ActualFanfoldHours', 'ActualGrossMargin_Percent', 'ActualMSI_StockRolls', 'ActualOtherLaborCost', 'ActualPostPressLaborCost', 'ActualTotalPOCosts', 'AmortizeColorChanges', 'AmortizePlateChanges', 'Are_Tools_for_Equip', 'ArtDone', 'Bill_TaxRegion_ID', 'CarrierWidth', 'ColorChangeCost', 'ColorChangeCost_Extended', 'ColumnPerf', 'CreditHoldOverride', 'Currency_ExchangeRate_x', 'Currency_Rate_ID', 'EarlyShipOK', 'Equip_Actual_MR_Length', 'Equip_Actual_WU_Hours', 'Equip_NumUp_Multiplier', 'Equip_WashUpHours', 'Equip3_Actual_Cost', 'Equip3_Actual_Hours', 'Equip3_Actual_Length', 'Equip3_Actual_MR_Hours', 'Equip3_Actual_MR_Length', 'Equip3_Actual_Rate', 'Equip3_Actual_Run_Hours', 'Equip3_Actual_Speed', 'Equip3_Actual_WU_Hours', 'Equip3_Done', 'Equip3_EstRunHrs', 'Equip3_EstSpeed', 'Equip3_EstTime', 'Equip3_MakeReadyHours', 'Equip3_NoAcross', 'Equip3_NoAround', 'Equip3_NumUp_Multiplier', 'Equip3_WashUpHours', 'Equip4_Actual_Cost', 'Equip4_Actual_Hours', 'Equip4_Actual_Length', 'Equip4_Actual_MR_Hours', 'Equip4_Actual_MR_Length', 'Equip4_Actual_Run_Hours', 'Equip4_Actual_Speed', 'Equip4_Actual_WU_Hours', 'Equip4_Done', 'Equip4_NoAcross', 'Equip4_NoAround', 'Equip4_NumUp_Multiplier', 'Equip4_WashUpHours', 'ESC_Art_Sales', 'ESC_Equip', 'ESC_Equip3', 'ESC_Equip3_Sales', 'ESC_Equip4', 'ESC_Equip4_Sales', 'ESC_Finish', 'ESC_Finish_Sales', 'ESC_Ink', 'ESC_Ink_Sales', 'ESC_Plate', 'ESC_Plate_Sales', 'ESC_Press_Sales', 'ESC_Ship_Sales', 'ESC_Stock', 'ESC_Stock_Sales', 'ESC_Tool_Sales', 'ESS_Ship', 'ESS_Ship_Sales', 'ESS_TickStat', 'ESS_TickStat_Sales', 'EstArtwork', 'EstPostPressHours', 'EstWuHrs', 'FC_ColorChangeCost', 'FC_ColorChangeCost_Extended', 'FC_Customer_Total', 'FC_EstTotal', 'FC_MiscCharge', 'FC_MiscCharge1', 'FC_MiscCharge2', 'FC_MiscCharge3', 'FC_MiscCharge4', 'FC_PlateChangeCost', 'FC_PlateChangeCost_Extended', 'FC_POTotal', 'FlexPack_Gusset', 'FlexPack_Height', 'FlexPack_LeftTrim', 'FlexPack_RightTrim', 'FlexPack_Type', 'Frames_Lead_Out', 'Is_ActBillNetTax_UserModified', 'Is_AutoOrder', 'Is_Ink_In', 'LabelsPerFold', 'MiscCharge1', 'MiscCharge2', 'MiscCharge3', 'MiscCharge4', 'NoColorChanges', 'POTotal', 'ProofDone', 'RollLength', 'RowPerf', 'Sheet_Height', 'Sheet_Width', 'Ship_TaxRegion_ID', 'ShipAsOne', 'ShrinkSleeve_CutHeight', 'ShrinkSleeve_LayFlat', 'ShrinkSleeve_OverLap', 'SoldToEndUser', 'StockProdDiscnt', 'StockTicketType', 'SubTicket', 'Tab', 'TabPosition', 'Tape', 'Total_LineWeight', 'TotalOrderWeight', 'TotalShipWeight', 'UserDef_MR_1', 'UserDef_MR_2', 'Art', 'Art_Item', 'Art_Ticket', 'CostM', 'Equip_NoColors', 'Equip_NoFloods', 'Equip_Null_Cycles', 'Equip3_NoColors', 'Equip3_NoFloods', 'Equip3_Null_Cycles', 'Equip4_NoColors', 'Equip4_NoFloods', 'Equip4_Null_Cycles', 'FC_LineTotal', 'FC_PriceM', 'Line_Weight', 'Plate_Item', 'Press_Null_Cycles', 'Proof', 'Proof_Item', 'Proof_Ticket', 'Unit_Weight', 'Commission', 'Currency_ExchangeRate_y', 'eTraxx_Forecast_Quantity', 'FC_Cost', 'InternetQuery', 'Inventory_Expires', 'Link_Factor', 'MaxProduce', 'MinProduce', 'Product_Image_Size', 'Production_Waste', 'Releases_Allowed_Default', 'TotalCost', 'Weight']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('----- DROP BOOLEAN AND NULL COLUMNS (NOT NEEDED) -----\\n')\n",
    "\n",
    "# drop columns with all rows as missing values (NaN/0)\n",
    "columns_to_drop = merged_data.columns[merged_data.isnull().all()].tolist()\n",
    "print(f'Merged Data - Columns to be dropped (Missing Vals): {columns_to_drop}\\n')\n",
    "merged_data = merged_data.dropna(axis=1, how='all')\n",
    "\n",
    "# set the name attribute for each DataFrame\n",
    "merged_data.name = 'Merged Data'\n",
    "\n",
    "# drop columns with all rows containing the same values of either 0, True, or False\n",
    "for df in [merged_data]:\n",
    "    columns_to_drop = [col for col in df.columns if df[col].nunique() == 1 and df[col].iloc[0] in [0, True, False]]\n",
    "    print(f'{df.name} - Columns to be dropped (All cols with rows same vals): {columns_to_drop}\\n')\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "print('-------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped TicQuantity column\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop 'TicQuantity' column as it is for a order (where orders can include multiple products)\n",
    "merged_data.drop(columns='TicQuantity', inplace=True)\n",
    "print('Dropped TicQuantity column\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows with 0 values in OrderQuantity column. Rows dropped: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows with '0' values in 'OrderQuantity' column\n",
    "merged_data = merged_data[merged_data.OrderQuantity != 0]\n",
    "\n",
    "# display a confirmation message with the rows dropped count\n",
    "print(f'Dropped rows with 0 values in OrderQuantity column. Rows dropped: {len(merged_data[merged_data.OrderQuantity == 0])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- MERGED DATA ----------------\n",
      "   Act_MakeReady_Footage  ActFootage  ActMRHrs  ActPackHrs  ActPressSpd  \\\n",
      "0                      0           0       0.0           0            0   \n",
      "1                      0           0       0.0           0            0   \n",
      "2                      0           0       0.0           0            0   \n",
      "3                      0           0       0.0           0            0   \n",
      "4                      0           0       0.0           0            0   \n",
      "\n",
      "   ActQuantity  ActRunHrs  ActStockCost  ActTotalCost  ActualFanfoldRate  ...  \\\n",
      "0            0        0.0           0.0           0.0                  0  ...   \n",
      "1            0        0.0           0.0           0.0                  0  ...   \n",
      "2            0        0.0           0.0           0.0                  0  ...   \n",
      "3            0        0.0           0.0          17.5                  0  ...   \n",
      "4            0        0.0           0.0           0.0                  0  ...   \n",
      "\n",
      "   PriceMode  PriceMode_Local_y             ProdClass   ProdSubClass  \\\n",
      "0      Per M              Per M    Country Pure Foods  Self Adhesive   \n",
      "1      Per M              Per M    Country Pure Foods  Self Adhesive   \n",
      "2      Per M              Per M    Country Pure Foods  Self Adhesive   \n",
      "3      Per M              Per M  Swift Prepared Foods  Self Adhesive   \n",
      "4      Per M              Per M    Uncle John's Pride  Self Adhesive   \n",
      "\n",
      "   Product_UniqueProdID  SupplierName  SupplierNo  SupplierPartNo   Updated  \\\n",
      "0               12822.0           NaN         NaN             NaN  00/00/00   \n",
      "1               27339.0           NaN         NaN             NaN  00/00/00   \n",
      "2               20161.0           NaN         NaN             NaN  00/00/00   \n",
      "3               21779.0           NaN         NaN             NaN  00/00/00   \n",
      "4               25542.0           NaN         NaN             NaN  00/00/00   \n",
      "\n",
      "        updateTimeDateStamp  \n",
      "0  2025-01-17T13:46:01.549Z  \n",
      "1  2025-01-17T13:46:03.637Z  \n",
      "2  2025-01-17T13:46:03.095Z  \n",
      "3  2025-01-17T14:53:02.886Z  \n",
      "4  2025-01-17T13:34:30.547Z  \n",
      "\n",
      "[5 rows x 253 columns]\n",
      "Index(['Act_MakeReady_Footage', 'ActFootage', 'ActMRHrs', 'ActPackHrs',\n",
      "       'ActPressSpd', 'ActQuantity', 'ActRunHrs', 'ActStockCost',\n",
      "       'ActTotalCost', 'ActualFanfoldRate',\n",
      "       ...\n",
      "       'PriceMode', 'PriceMode_Local_y', 'ProdClass', 'ProdSubClass',\n",
      "       'Product_UniqueProdID', 'SupplierName', 'SupplierNo', 'SupplierPartNo',\n",
      "       'Updated', 'updateTimeDateStamp'],\n",
      "      dtype='object', length=253)\n",
      "-------------------------------------------\n",
      "-------------- SUPPLIER NAMES ----------------\n",
      "['Nilpeter Inc']\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# display merged data after dropping columns\n",
    "print('-------------- MERGED DATA ----------------')\n",
    "print(merged_data.head())\n",
    "# display ALL columns\n",
    "print(merged_data.columns)\n",
    "print('-------------------------------------------')\n",
    "\n",
    "# print all customername and suppliername which arent null in seperate dfs\n",
    "# print('-------------- CUSTOMER NAMES ----------------')\n",
    "# print(merged_data[merged_data.CustomerName.notnull()]['CustomerName'].unique())\n",
    "# print('----------------------------------------------')\n",
    "\n",
    "print('-------------- SUPPLIER NAMES ----------------')\n",
    "print(merged_data[merged_data.SupplierName.notnull()]['SupplierName'].unique())\n",
    "print('----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'], errors='coerce')\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'], errors='coerce')\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:10: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  merged_data['Ship_by_Date'] = pd.to_datetime(merged_data['Ship_by_Date'], errors='coerce')\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['Ship_by_Date'] = pd.to_datetime(merged_data['Ship_by_Date'], errors='coerce')\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['order_date'] = merged_data['OrderDate']\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['ship_by_date'] = merged_data['Ship_by_Date']\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['order_month'] = merged_data['order_date'].dt.month\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['order_week'] = merged_data['order_date'].dt.isocalendar().week\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['order_year'] = merged_data['order_date'].dt.year\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['order_weekday'] = merged_data['OrderDate'].dt.weekday\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['is_weekend'] = (merged_data['order_weekday'] >= 5).astype(int)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['quarter'] = merged_data['OrderDate'].dt.quarter\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['is_end_of_month'] = (merged_data['OrderDate'].dt.day > 25).astype(int)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['prev_year_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(12)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['prev_week_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(1)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['yoy_growth'] = (merged_data['OrderQuantity'] - merged_data['prev_year_sales']) / merged_data['prev_year_sales']\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['sales_2022'] = merged_data.apply(lambda x: x['OrderQuantity'] if x['order_year'] == 2022 else 0, axis=1)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['sales_2023'] = merged_data.apply(lambda x: x['OrderQuantity'] if x['order_year'] == 2023 else 0, axis=1)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['sales_2024'] = merged_data.apply(lambda x: x['OrderQuantity'] if x['order_year'] == 2024 else 0, axis=1)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['growth_2023'] = (merged_data['sales_2023'] - merged_data['sales_2022']) / merged_data['sales_2022'] * 100\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['growth_2024'] = (merged_data['sales_2024'] - merged_data['sales_2023']) / merged_data['sales_2023'] * 100\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['prev_month_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(1)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['prev_2_month_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(2)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['prev_3_month_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(3)\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['moving_avg_3m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['moving_avg_6m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=6, min_periods=1).mean())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['moving_avg_12m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['moving_avg_18m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=18, min_periods=1).mean())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['var_1m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=1, min_periods=1).var())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['var_3m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=3, min_periods=1).var())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['var_6m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=6, min_periods=1).var())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['var_12m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=12, min_periods=1).var())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['var_18m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=18, min_periods=1).var())\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['log_var_1m'] = np.log1p(merged_data['var_1m'])  # log1p prevents log(0) errors\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['log_var_3m'] = np.log1p(merged_data['var_3m'])\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['log_var_6m'] = np.log1p(merged_data['var_6m'])\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['log_var_12m'] = np.log1p(merged_data['var_12m'])\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['log_var_18m'] = np.log1p(merged_data['var_18m'])\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/3782586238.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data[col] = merged_data[col].fillna(merged_data[col].mean())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "TRYING DIFFERENT FEATURES TO SEE HOW THEY ACT WITH MODEL PERFORMANCE\n",
    "'''\n",
    "\n",
    "# ensure date columns are properly formatted\n",
    "merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'], errors='coerce')\n",
    "merged_data['Ship_by_Date'] = pd.to_datetime(merged_data['Ship_by_Date'], errors='coerce')\n",
    "merged_data['order_date'] = merged_data['OrderDate']\n",
    "merged_data['ship_by_date'] = merged_data['Ship_by_Date']\n",
    "\n",
    "# time-based features\n",
    "merged_data['order_month'] = merged_data['order_date'].dt.month\n",
    "merged_data['order_week'] = merged_data['order_date'].dt.isocalendar().week\n",
    "merged_data['order_year'] = merged_data['order_date'].dt.year\n",
    "merged_data['order_weekday'] = merged_data['OrderDate'].dt.weekday\n",
    "merged_data['is_weekend'] = (merged_data['order_weekday'] >= 5).astype(int)\n",
    "merged_data['quarter'] = merged_data['OrderDate'].dt.quarter\n",
    "merged_data['is_end_of_month'] = (merged_data['OrderDate'].dt.day > 25).astype(int)\n",
    "\n",
    "# year-over-year growth (yoy)\n",
    "merged_data['prev_year_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(12)\n",
    "merged_data['prev_week_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(1)\n",
    "merged_data['yoy_growth'] = (merged_data['OrderQuantity'] - merged_data['prev_year_sales']) / merged_data['prev_year_sales']\n",
    "\n",
    "merged_data['sales_2022'] = merged_data.apply(lambda x: x['OrderQuantity'] if x['order_year'] == 2022 else 0, axis=1)\n",
    "merged_data['sales_2023'] = merged_data.apply(lambda x: x['OrderQuantity'] if x['order_year'] == 2023 else 0, axis=1)\n",
    "merged_data['sales_2024'] = merged_data.apply(lambda x: x['OrderQuantity'] if x['order_year'] == 2024 else 0, axis=1)\n",
    "\n",
    "# growth of product sales per year (%)\n",
    "merged_data['growth_2023'] = (merged_data['sales_2023'] - merged_data['sales_2022']) / merged_data['sales_2022'] * 100\n",
    "merged_data['growth_2024'] = (merged_data['sales_2024'] - merged_data['sales_2023']) / merged_data['sales_2023'] * 100\n",
    " \n",
    "\n",
    "# lag features (considers past trends via products)\n",
    "merged_data['prev_month_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(1)\n",
    "merged_data['prev_2_month_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(2)\n",
    "merged_data['prev_3_month_sales'] = merged_data.groupby('ProductNumber')['OrderQuantity'].shift(3)\n",
    "\n",
    "# time difference features\n",
    "# merged_data['days_since_last_order'] = merged_data.groupby('ProductNumber')['OrderDate'].diff().dt.days.fillna(30)\n",
    "\n",
    "# rolling features\n",
    "merged_data['moving_avg_3m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "merged_data['moving_avg_6m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=6, min_periods=1).mean())\n",
    "merged_data['moving_avg_12m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=12, min_periods=1).mean())\n",
    "merged_data['moving_avg_18m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=18, min_periods=1).mean())\n",
    "\n",
    "merged_data['var_1m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=1, min_periods=1).var())\n",
    "merged_data['var_3m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=3, min_periods=1).var())\n",
    "merged_data['var_6m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=6, min_periods=1).var())\n",
    "merged_data['var_12m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=12, min_periods=1).var())\n",
    "merged_data['var_18m'] = merged_data.groupby('ProductNumber')['OrderQuantity'].transform(lambda x: x.rolling(window=18, min_periods=1).var())\n",
    "\n",
    "# log-transformed variance\n",
    "merged_data['log_var_1m'] = np.log1p(merged_data['var_1m'])  # log1p prevents log(0) errors\n",
    "merged_data['log_var_3m'] = np.log1p(merged_data['var_3m'])\n",
    "merged_data['log_var_6m'] = np.log1p(merged_data['var_6m'])\n",
    "merged_data['log_var_12m'] = np.log1p(merged_data['var_12m'])\n",
    "merged_data['log_var_18m'] = np.log1p(merged_data['var_18m'])\n",
    "\n",
    "# check and fill missing values\n",
    "for col in [\n",
    "    'prev_month_sales', 'prev_week_sales', \n",
    "    'moving_avg_3m', 'moving_avg_6m', 'moving_avg_12m', 'moving_avg_18m',\n",
    "    'prev_2_month_sales', 'prev_3_month_sales',\n",
    "    'var_1m', 'var_3m', 'var_6m', 'var_12m', 'var_18m'\n",
    "    ]:\n",
    "    merged_data[col] = merged_data[col].fillna(merged_data[col].mean())\n",
    "\n",
    "# # interaction Features\n",
    "# merged_data['interaction_1'] = merged_data['prev_month_sales'] * merged_data['var_12m']\n",
    "# merged_data['interaction_2'] = merged_data['prev_week_sales'] * merged_data['var_12m']\n",
    "# merged_data['interaction_3'] = merged_data['moving_avg_3m'] * merged_data['moving_avg_12m']\n",
    "# merged_data['interaction_4'] = merged_data['prev_2_month_sales'] * merged_data['var_18m']\n",
    "# merged_data['interaction_5'] = merged_data['prev_3_month_sales'] * merged_data['var_18m']\n",
    "\n",
    "# # demand Factors\n",
    "# merged_data['inventory_ratio'] = merged_data['PhysicalInv'] / (merged_data['OnOrder'] + 1)\n",
    "# merged_data['is_backordered'] = merged_data['BackOrdered'].notna().astype(int)\n",
    "# merged_data['customer_order_count'] = merged_data.groupby('Customer_Num')['OrderQuantity'].transform('count')\n",
    "# merged_data['customer_avg_order'] = merged_data.groupby('Customer_Num')['OrderQuantity'].transform('mean')\n",
    "\n",
    "# aggregation\n",
    "product_sales = merged_data.groupby([ # group rows by:\n",
    "    'ProductNumber', 'order_year', 'order_month', 'order_week',\n",
    "    'Customer_Num'\n",
    "]).agg({ # include these columns with respective data \n",
    "    'OrderQuantity': 'sum', \n",
    "    'prev_month_sales': 'mean',\n",
    "    'prev_week_sales': 'mean',\n",
    "    'prev_2_month_sales': 'mean',\n",
    "    'prev_3_month_sales': 'mean',\n",
    "    'var_1m': 'mean',\n",
    "    'var_3m': 'mean',\n",
    "    'var_6m': 'mean',\n",
    "    'var_12m': 'mean',\n",
    "    'var_18m': 'mean',\n",
    "    'log_var_1m': 'mean',\n",
    "    'log_var_3m': 'mean',\n",
    "    'log_var_6m': 'mean',\n",
    "    'log_var_12m': 'mean',\n",
    "    'log_var_18m': 'mean',\n",
    "    'yoy_growth': 'mean',\n",
    "    'moving_avg_3m': 'mean',\n",
    "    'moving_avg_6m': 'mean',\n",
    "    'moving_avg_12m': 'mean',\n",
    "    'moving_avg_18m': 'mean',\n",
    "    'sales_2022': 'sum',\n",
    "    'sales_2023': 'sum',\n",
    "    'sales_2024': 'sum',\n",
    "    'growth_2023': 'mean',\n",
    "    'growth_2024': 'mean',\n",
    "    }).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/1541869033.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sales_data = merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'])\n",
      "/var/folders/31/9bd8ksys1rzbjk8qs9scl95h0000gn/T/ipykernel_61970/1541869033.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'])\n"
     ]
    }
   ],
   "source": [
    "# add the more feature columns to the product_sales dataset\n",
    "product_sales = pd.merge(product_sales, products_s[['ProductNumber', 'PhysicalInv']], on='ProductNumber')\n",
    "\n",
    "# sales data for SARIMAX time series model test\n",
    "# sales_data = merged_data.groupby(['OrderDate'])['OrderQuantity'].sum()\n",
    "\n",
    "sales_data = merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'])\n",
    "\n",
    "# 'OrderDate' to datetime\n",
    "merged_data['OrderDate'] = pd.to_datetime(merged_data['OrderDate'])\n",
    "\n",
    "# index correctly\n",
    "sales_data = merged_data.set_index(['OrderDate', 'ProductNumber'])\n",
    "sales_data = sales_data['OrderQuantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>order_year</th>\n",
       "      <th>order_month</th>\n",
       "      <th>order_week</th>\n",
       "      <th>Customer_Num</th>\n",
       "      <th>OrderQuantity</th>\n",
       "      <th>prev_month_sales</th>\n",
       "      <th>prev_week_sales</th>\n",
       "      <th>prev_2_month_sales</th>\n",
       "      <th>prev_3_month_sales</th>\n",
       "      <th>...</th>\n",
       "      <th>moving_avg_3m</th>\n",
       "      <th>moving_avg_6m</th>\n",
       "      <th>moving_avg_12m</th>\n",
       "      <th>moving_avg_18m</th>\n",
       "      <th>sales_2022</th>\n",
       "      <th>sales_2023</th>\n",
       "      <th>sales_2024</th>\n",
       "      <th>growth_2023</th>\n",
       "      <th>growth_2024</th>\n",
       "      <th>PhysicalInv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFS-005-0003S</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>123553.037584</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFS-005-0003S</td>\n",
       "      <td>2024</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>116875.76046</td>\n",
       "      <td>123553.037584</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFS-005-0003S</td>\n",
       "      <td>2024</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>108469.569193</td>\n",
       "      <td>108469.569193</td>\n",
       "      <td>116875.76046</td>\n",
       "      <td>123553.037584</td>\n",
       "      <td>...</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>inf</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFS-005-0003U</td>\n",
       "      <td>2023</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>196.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>20000.00000</td>\n",
       "      <td>123553.037584</td>\n",
       "      <td>...</td>\n",
       "      <td>10833.333333</td>\n",
       "      <td>10833.333333</td>\n",
       "      <td>10833.333333</td>\n",
       "      <td>10833.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFS-005-0003U</td>\n",
       "      <td>2023</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>196.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>116875.76046</td>\n",
       "      <td>123553.037584</td>\n",
       "      <td>...</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>15000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductNumber  order_year  order_month  order_week  Customer_Num  \\\n",
       "0  AFS-005-0003S        2024            5          20         196.0   \n",
       "1  AFS-005-0003S        2024            9          38         196.0   \n",
       "2  AFS-005-0003S        2024           10          41         196.0   \n",
       "3  AFS-005-0003U        2023            9          39         196.0   \n",
       "4  AFS-005-0003U        2023           11          45         196.0   \n",
       "\n",
       "   OrderQuantity  prev_month_sales  prev_week_sales  prev_2_month_sales  \\\n",
       "0          10000      10000.000000     10000.000000         10000.00000   \n",
       "1          10000      10000.000000     10000.000000        116875.76046   \n",
       "2          10000     108469.569193    108469.569193        116875.76046   \n",
       "3           2500      10000.000000     10000.000000         20000.00000   \n",
       "4          10000      20000.000000     20000.000000        116875.76046   \n",
       "\n",
       "   prev_3_month_sales  ...  moving_avg_3m  moving_avg_6m  moving_avg_12m  \\\n",
       "0       123553.037584  ...   10000.000000   10000.000000    10000.000000   \n",
       "1       123553.037584  ...   10000.000000   10000.000000    10000.000000   \n",
       "2       123553.037584  ...   10000.000000   10000.000000    10000.000000   \n",
       "3       123553.037584  ...   10833.333333   10833.333333    10833.333333   \n",
       "4       123553.037584  ...   15000.000000   15000.000000    15000.000000   \n",
       "\n",
       "   moving_avg_18m  sales_2022  sales_2023  sales_2024  growth_2023  \\\n",
       "0    10000.000000           0           0       10000          NaN   \n",
       "1    10000.000000           0           0       10000          NaN   \n",
       "2    10000.000000           0           0       10000          NaN   \n",
       "3    10833.333333           0        2500           0          inf   \n",
       "4    15000.000000           0       10000           0          inf   \n",
       "\n",
       "   growth_2024  PhysicalInv  \n",
       "0          inf            0  \n",
       "1          inf            0  \n",
       "2          inf            0  \n",
       "3       -100.0            0  \n",
       "4       -100.0            0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves outputs of products_sales to '../datasets/stock_forecasting/final'\n",
    "product_sales.to_csv('../datasets/stock_forecasting/final/product_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderDate   ProductNumber\n",
       "2025-01-17  CPF-026-0457Z    30000\n",
       "            CPF-026-0458X    30000\n",
       "            CPF-026-0468Y    30000\n",
       "            SWI-005-0026Z    45000\n",
       "            UJP-007-0005Y    60000\n",
       "Name: OrderQuantity, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all columns from all datasets and save them to a txt file (products_s, tickets_c_i, tickets_c_m)\n",
    "# extract columns from each dataset and save them to a txt file one by one\n",
    "columns = pd.DataFrame()\n",
    "\n",
    "# concatenate columns from products_s\n",
    "columns = pd.concat([columns, pd.DataFrame(products_s.columns, columns=['Column'])])\n",
    "\n",
    "# concatenate columns from tickets_c_i\n",
    "columns = pd.concat([columns, pd.DataFrame(tickets_c_i.columns, columns=['Column'])])\n",
    "\n",
    "# concatenate columns from tickets_c_m\n",
    "columns = pd.concat([columns, pd.DataFrame(tickets_c_m.columns, columns=['Column'])])\n",
    "\n",
    "# concatenate columns from sp_inv_adds\n",
    "columns = pd.concat([columns, pd.DataFrame(sp_inv_adds.columns, columns=['Column'])])\n",
    "\n",
    "# concatenate columns from sp_inv_rel\n",
    "columns = pd.concat([columns, pd.DataFrame(sp_inv_rel.columns, columns=['Column'])])\n",
    "\n",
    "# save the columns to a txt file\n",
    "# columns.to_csv('../datasets/stock_forecasting/2022-2025/columns.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
